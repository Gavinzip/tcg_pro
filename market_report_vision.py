import argparse
import subprocess
import re
import requests
import json
import time
import urllib.parse
import concurrent.futures
import os
import base64
import asyncio
import threading
import image_generator
import tempfile
from collections import deque
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

REPORT_ONLY = False
import contextvars
# Use ContextVar for thread-safe per-task debug directory
_debug_dir_var = contextvars.ContextVar('DEBUG_DIR', default=None)

def _get_debug_dir():
    return _debug_dir_var.get()

def _set_debug_dir(path):
    _debug_dir_var.set(path)

def _debug_save(filename, content):
    """Debug è¼”åŠ©å‡½æ•¸ï¼šå°‡å…§å®¹å­˜å…¥ DEBUG_DIR/filenameï¼ˆè‹¥ DEBUG_DIR å·²è¨­å®šï¼‰"""
    debug_dir = _get_debug_dir()
    if not debug_dir:
        return
    os.makedirs(debug_dir, exist_ok=True)
    filepath = os.path.join(debug_dir, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    _original_print(f"  ğŸ’¾ [DEBUG] å­˜æª”: {filepath}")

def _debug_log(msg):
    """Debug log è¼”åŠ©å‡½æ•¸ï¼šå°‡è¨Šæ¯ append åˆ° DEBUG_DIR/debug_log.txt"""
    debug_dir = _get_debug_dir()
    if not debug_dir:
        return
    os.makedirs(debug_dir, exist_ok=True)
    timestamp = time.strftime('%H:%M:%S')
    line = f"[{timestamp}] {msg}\n"
    _original_print(f"  ğŸ“ [DEBUG] {msg}")
    with open(os.path.join(debug_dir, 'debug_log.txt'), 'a', encoding='utf-8') as f:
        f.write(line)

def _debug_step(source: str, step_num: int, query: str, url: str,
                status: str, candidate_urls: list = None,
                selected_url: str = None, reason: str = "",
                extra: dict = None):
    """
    çµæ§‹åŒ– Debug Trace â€” æ¯æ¬¡æœå°‹å‹•ä½œéƒ½è¨˜éŒ„ä¸€ç­† JSON åˆ° debug_trace.jsonl
    """
    debug_dir = _get_debug_dir()
    if not debug_dir:
        return
    os.makedirs(debug_dir, exist_ok=True)
    record = {
        "time": time.strftime('%H:%M:%S'),
        "source": source,
        "step": step_num,
        "query": query,
        "url": url,
        "status": status,
        "candidate_urls": candidate_urls or [],
        "selected_url": selected_url or "",
        "reason": reason,
    }
    if extra:
        record.update(extra)
    # å³æ™‚ print åˆ° terminal
    icon = "âœ…" if status == "OK" else "âŒ"
    _original_print(f"  {icon} [{source} Step {step_num}] query={query!r}")
    _original_print(f"       URL  : {url}")
    _original_print(f"       ç‹€æ…‹ : {status}  â€”  {reason}")
    if candidate_urls:
        _original_print(f"       å€™é¸ URLs ({len(candidate_urls)} ç­†):")
        for u in candidate_urls:
            _original_print(f"         â€¢ {u}")
    if selected_url:
        _original_print(f"       é¸å®š URL : {selected_url}")
    # append åˆ° JSONL
    with open(os.path.join(debug_dir, 'debug_trace.jsonl'), 'a', encoding='utf-8') as f:
        f.write(json.dumps(record, ensure_ascii=False) + '\n')

_original_print = print
def print(*args, **kwargs):
    if REPORT_ONLY and not kwargs.get('force', False):
        return
    if 'force' in kwargs:
        del kwargs['force']
    _original_print(*args, **kwargs)

_jina_requests_queue = deque()
_jina_lock = threading.Lock()

def fetch_jina_markdown(target_url):
    global _jina_requests_queue
    
    # Rate Limiter: 18 requests per 60 seconds (1 minute)
    MAX_REQUESTS = 18
    WINDOW_SIZE = 60.0
    
    sleep_time = 0
    with _jina_lock:
        now = time.time()
        # Remove requests older than 60 seconds
        while _jina_requests_queue and now - _jina_requests_queue[0] > WINDOW_SIZE:
            _jina_requests_queue.popleft()
            
        if len(_jina_requests_queue) >= MAX_REQUESTS:
            # Calculate sleep time required to let the oldest request expire
            sleep_time = WINDOW_SIZE - (now - _jina_requests_queue[0])
    
    if sleep_time > 0:
        print(f"â³ Jina API rate limit approaching ({MAX_REQUESTS}/min). Pausing for {sleep_time:.1f} seconds to cool down...")
        time.sleep(sleep_time)
        
    # Re-acquire lock to record the actual request time
    with _jina_lock:
        now = time.time()
        # Clean up again just in case another thread already cleaned up during our sleep
        while _jina_requests_queue and now - _jina_requests_queue[0] > WINDOW_SIZE:
            _jina_requests_queue.popleft()
        _jina_requests_queue.append(now)

    print(f"Fetching: {target_url}...")
    jina_url = f"https://r.jina.ai/{target_url}"
    
    for attempt in range(3):
        try:
            response = requests.get(jina_url, timeout=60)
            if response.status_code == 429:
                print(f"âš ï¸ Jina ç™¼ç”Ÿ 429 é »ç‡é™åˆ¶ (å˜—è©¦ {attempt+1}/3). æš«åœ 1 ç§’å¾Œé‡è©¦...")
                time.sleep(1)
                continue
                
            response.raise_for_status()
            return response.text
            
        except requests.exceptions.RequestException as e:
            if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                print(f"âš ï¸ Jina ç™¼ç”Ÿ 429 é »ç‡é™åˆ¶ (å˜—è©¦ {attempt+1}/3). æš«åœ 1 ç§’å¾Œé‡è©¦...")
                time.sleep(1)
                continue
                
            print(f"Fetch error for {target_url}: {e}")
            return ""
            
    return ""

def get_exchange_rate():
    try:
        resp = requests.get("https://open.er-api.com/v6/latest/USD")
        data = resp.json()
        return data['rates']['JPY']
    except:
        return 150.0

def extract_price(price_str):
    cleaned = re.sub(r'[^\d.]', '', price_str)
    try:
        return float(cleaned)
    except:
        return 0.0

def _fetch_pc_prices_from_url(product_url, md_content=None, skip_hi_res=False):
    """
    Given a PriceCharting product URL, fetch (if md_content is None) and parse it.
    Returns (records, resolved_url, pc_img_url).
    """
    if not md_content:
        md_content = fetch_jina_markdown(product_url)
    
    if not md_content:
        print(f"DEBUG: Failed to get markdown for {product_url}")
        return [], product_url, None

    print(f"DEBUG: Parsing PriceCharting page: {product_url} (length: {len(md_content)})")

    lines = md_content.split('\n')
    records = []
    
    # Parser 1: å˜—è©¦åŸæœ¬çš„ Markdown Table æ ¼å¼ (æ¯è¡Œæœ‰ | åˆ†éš”)
    date_regex_md = r'\|\s*(\d{4}-\d{2}-\d{2}|[A-Z][a-z]{2}\s\d{1,2},\s\d{4})\s*\|'
    for line in lines:
        if re.search(date_regex_md, line):
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 5:
                date_str = parts[1]
                all_prices = re.findall(r'\$([\d,]+\.\d{2})', line)
                if not all_prices: continue
                real_prices = [p for p in all_prices if p not in ('6.00',)]
                if not real_prices: continue
                
                price_usd = float(real_prices[-1].replace(',', ''))
                title_clean = line.replace(" ", "").lower()
                
                detected_grade = None
                if re.search(r'(psa|cgc|bgs|grade|gem)10', title_clean) or ("psa" in title_clean and "10" in title_clean):
                    detected_grade = "PSA 10"
                elif re.search(r'bgs\s*9\.5', title_clean):
                    detected_grade = "BGS 9.5"
                elif re.search(r'(psa|cgc|bgs|grade|gem)9', title_clean) or ("psa" in title_clean and "9" in title_clean):
                    detected_grade = "PSA 9"
                elif re.search(r'(psa|cgc|bgs|grade|gem)8', title_clean) or ("psa" in title_clean and "8" in title_clean):
                    detected_grade = "PSA 8"
                elif not re.search(r'(psa|bgs|cgc|grade|gem)', title_clean):
                    detected_grade = "Ungraded"
                        
                if detected_grade:
                    records.append({
                        "date": date_str,
                        "price": price_usd,
                        "grade": detected_grade
                    })

    # Parser 2: å˜—è©¦ Jina æ–°ç‰ˆçš„ TSV æ ¼å¼ (æ—¥æœŸç¨ç«‹ä¸€è¡Œï¼Œæ¨™é¡Œèˆ‡åƒ¹æ ¼åœ¨ä¸‹ä¸€è¡Œ)
    if not records:
        current_date = None
        date_regex_tsv = r'^(\d{4}-\d{2}-\d{2}|[A-Z][a-z]{2}\s\d{1,2},\s\d{4})'
        for line in lines:
            line = line.strip()
            date_match = re.match(date_regex_tsv, line)
            if date_match:
                current_date = date_match.group(1)
                continue
            if current_date and "$" in line:
                all_prices = re.findall(r'\$([\d,]+\.\d{2})', line)
                if not all_prices: continue
                real_prices = [p for p in all_prices if p not in ('6.00',)]
                if not real_prices: continue
                price_usd = float(real_prices[-1].replace(',', ''))
                title_clean = line.replace(" ", "").lower()
                detected_grade = None
                if re.search(r'(psa|cgc|bgs|grade|gem)10', title_clean) or ("psa" in title_clean and "10" in title_clean):
                    detected_grade = "PSA 10"
                elif re.search(r'(psa|cgc|bgs|grade|gem)9', title_clean) or ("psa" in title_clean and "9" in title_clean):
                    detected_grade = "PSA 9"
                elif re.search(r'(psa|cgc|bgs|grade|gem)8', title_clean) or ("psa" in title_clean and "8" in title_clean):
                    detected_grade = "PSA 8"
                elif not re.search(r'(psa|bgs|cgc|grade|gem)', title_clean):
                    detected_grade = "Ungraded"
                if detected_grade:
                    records.append({
                        "date": current_date,
                        "price": price_usd,
                        "grade": detected_grade
                    })

    # Summary prices
    today_str = datetime.now().strftime('%Y-%m-%d')
    grade_summary_map = {'Ungraded': 'Ungraded', 'PSA 10': 'PSA 10', 'PSA 9': 'PSA 9', 'PSA 8': 'PSA 8'}
    existing_grades = set(r['grade'] for r in records)
    for line in lines:
        for grade_label, grade_key in grade_summary_map.items():
            label_nospace = grade_label.replace(' ', '')
            if re.match(rf'^{re.escape(label_nospace)}\$[\d,]+\.\d{{2}}$', line.replace(' ', '')):
                if grade_key not in existing_grades:
                    price_match = re.search(r'\$[\d,]+\.\d{2}', line)
                    if price_match:
                        price_usd = extract_price(price_match.group(0))
                        records.append({"date": today_str, "price": price_usd, "grade": grade_key, "note": "PC avg price"})
    
    records.sort(key=lambda x: x['date'], reverse=True)
    
    pc_img_url = None
    # æ“´å±• regex ä»¥åŒ¹é…æ›´å¤šå¯èƒ½çš„åœ–ç‰‡è·¯å¾‘æ ¼å¼
    img_patterns = [
        r'!\[.*?\]\((https://storage\.googleapis\.com/images\.pricecharting\.com/[^/)]+/\d+\.jpg)\)',
        r'!\[.*?\]\((https://product-images\.s3\.amazonaws\.com/[^\)]+)\)',
        r'!\[.*?\]\((https://images\.pricecharting\.com/[^\)]+)\)',
        r'!\[.*?\]\((https://[^)]+?pricecharting\.com/[^)]+?\.(?:jpg|png|webp)[^)]*)\)',
        r'!\[.*?\]\((https://[^)]+?\.(?:jpg|png|webp)[^)]*)\)',
    ]
    for pat in img_patterns:
        m = re.search(pat, md_content)
        if m:
            pc_img_url = m.group(1)
            print(f"DEBUG: Found image URL: {pc_img_url}")
            if not skip_hi_res:
                hiRes_url = re.sub(r'/([\d]+)\.jpg$', '/1600.jpg', pc_img_url)
                if hiRes_url != pc_img_url:
                    try:
                        if requests.head(hiRes_url, timeout=5).status_code == 200:
                            pc_img_url = hiRes_url
                            print(f"DEBUG: Upgraded to 1600px: {pc_img_url}")
                    except: pass
            break
            
    if not pc_img_url:
        print(f"DEBUG: No image found in markdown for {product_url}")

    return records, product_url, pc_img_url

def search_pricecharting(name, number, set_code, is_alt_art=False, category="Pokemon"):
    # Basic Name cleaning (strip parentheses like "Queen (Flagship Battle Top 8 Prize)")
    name_query = re.sub(r'\(.*?\)', '', name).strip()

    # Improve number extraction for One Piece (ST04-005 -> 005, OP02-026 -> 026)
    # If the number contains a dash and follows OP/ST format, take the part after the dash
    if '-' in number and re.search(r'[A-Z]+\d+-\d+', number):
        number_clean = number.split('-')[-1].lstrip('0')
    else:
        _num_parts = number.split('/')
        _num_raw = _num_parts[0].strip()
        _digits_only = re.search(r'\d+', _num_raw)
        number_clean = _digits_only.group(0).lstrip('0') if _digits_only else _num_raw.lstrip('0')

    if not number_clean: number_clean = '0'

    # Try to extract suffix like SM-P from the number itself if it's there
    suffix = ""
    _num_parts = number.split('/')
    if len(_num_parts) > 1:
        potential_suffix = _num_parts[1].strip()
        if re.search(r'(SM-P|S-P|SV-P|SV-G|S8a-G)', potential_suffix, re.IGNORECASE):
            suffix = potential_suffix

    # Try with set code or suffix first
    queries_to_try = []
    final_set_code = set_code if set_code else suffix

    if final_set_code:
        queries_to_try.append(f"{name_query} {final_set_code} {number_clean}".replace(" ", "+"))

    queries_to_try.append(f"{name_query} {number_clean}".replace(" ", "+"))

    is_one_piece = category.lower() == "one piece"

    md_content = ""
    search_url = ""

    for query in queries_to_try:
        search_url = f"https://www.pricecharting.com/search-products?q={query}&type=prices"
        md_content = fetch_jina_markdown(search_url)
        if md_content and ("Search Results" in md_content or "Your search for" in md_content):
            break
        elif md_content and "PriceCharting" in md_content:
            # might have landed on product directly
            break

    if not md_content:
        return None, None

    product_url = None

    if "Your search for" in md_content or "Search Results" in md_content:
        urls = re.findall(r'(https://www\.pricecharting\.com/game/[^/]+/[^" )\]]+)', md_content)
        # Deduplicate while preserving order
        urls = list(dict.fromkeys(urls))

        valid_urls = []
        # ã€Œåç¨± slugã€ç”¨ç´”è§’è‰²åï¼ˆå»æ‰æ‹¬è™Ÿå…§çš„ç‰ˆæœ¬æè¿°ï¼Œå¦‚ Leader Parallel / SP Foil ç­‰ï¼‰
        name_for_slug = re.sub(r'\(.*?\)', '', name).strip()
        name_slug = re.sub(r'[^a-zA-Z0-9]', '-', name_for_slug.lower()).strip('-')
        # ç·¨è™Ÿçš„ 0-padded 3ä½å½¢å¼ï¼Œä¿®å¾© URL slug å…§ 026 ä¸èƒ½è¢« 26 regex åŒ¹é…çš„å•é¡Œ
        number_padded_pc = number_clean.zfill(3)
        # èˆªæµ·ç‹æ¨¡å¼ï¼šset_code slug ç”¨ä¾†åšé¡å¤–é©—è­‰ (e.g. "OP02" -> "op02")
        set_code_slug = re.sub(r'[^a-zA-Z0-9]', '', set_code).lower() if set_code else ""

        def _num_match(slug):
            """ç·¨è™ŸåŒ¹é…ï¼šæ¥å—å»å‰å°0 æˆ– 3ä½è£œé½Šå…©ç¨®å½¢å¼"""
            return (bool(re.search(rf'(?<!\d){number_clean}(?!\d)', slug))
                    or number_padded_pc in slug)

        def _set_match(slug):
            """set_code åŒ¹é…ï¼šURL slug å«æœ‰ set_code çš„æ ¸å¿ƒå­—æ¯æ•¸å­—éƒ¨åˆ†"""
            return bool(set_code_slug) and set_code_slug in slug.replace('-', '')

        matching_both = []   # åç¨± + ç·¨è™Ÿ (+ set_code for OP)
        matching_name = []   # åªæœ‰åç¨± (+ set_code for OP)
        matching_number = [] # åªæœ‰ç·¨è™Ÿ (+ set_code for OP)

        for u in urls:
            u_end = u.split('/')[-1].lower()

            if is_one_piece:
                # â”€â”€ èˆªæµ·ç‹æ¨¡å¼ï¼šå¿…é ˆåŒ…å« set_codeï¼Œå†ä¾åç¨±/ç·¨è™Ÿåˆ†ç´š â”€â”€
                has_set = _set_match(u_end)
                has_num = _num_match(u_end)
                has_name = bool(name_slug) and name_slug in u_end

                if has_name and has_num and has_set:
                    matching_both.append(u)
                elif has_name and has_set:
                    matching_name.append(u)
                elif has_num and has_set:
                    matching_number.append(u)
                elif has_name and has_num:
                    # set_code æ²’ä¸­ä½†åç¨±+ç·¨è™Ÿéƒ½æœ‰ â†’ åˆ—ç‚ºå‚™é¸
                    matching_number.append(u)
            else:
                # â”€â”€ å¯¶å¯å¤¢æ¨¡å¼ï¼šåŸæœ¬çš„ 3 å±¤é‚è¼¯ä¸è®Š â”€â”€
                if name_slug and name_slug in u_end and _num_match(u_end):
                    matching_both.append(u)
                elif name_slug and name_slug in u_end:
                    matching_name.append(u)
                elif _num_match(u_end):
                    matching_number.append(u)

        # åˆä½µï¼šæœ€é«˜å„ªå…ˆç‚ºåŒæ™‚ç¬¦åˆçš„ï¼Œä¾åºéæ¸›
        valid_urls = matching_both + matching_name + matching_number

        if not valid_urls:
            _debug_step("PriceCharting", 1, name_slug, search_url, "NO_MATCH", reason=f"æ²’æœ‰ç¬¦åˆå¡ç‰‡åç¨±æˆ–ç·¨è™Ÿçš„ URL")
            print(f"DEBUG: No PC product URL matched the card name '{name}' or number '{number_clean}'.")
            return None, None, None, []

        # â”€â”€ èˆªæµ·ç‹ç‰ˆæœ¬é¸æ“‡é‚è¼¯ â”€â”€
        # å¦‚æœæ˜¯èˆªæµ·ç‹ï¼Œä¸”æœ‰å¤šå€‹åŒæ™‚ç¬¦åˆã€Œåç¨±+ç·¨è™Ÿ+SetCodeã€çš„ URLï¼Œä¸”ä¸æ˜¯ Alt-Art æ˜ç¢ºæ¨™ç¤ºï¼Œå‰‡è¿”å›å¾…é¸æ¸…å–®
        if is_one_piece and len(matching_both) > 1:
            _debug_step("PriceCharting", 1, name_slug, search_url, "AMBIGUOUS", candidate_urls=matching_both, reason="åµæ¸¬åˆ°å¤šå€‹èˆªæµ·ç‹å€™é¸ç‰ˆæœ¬")
            print(f"DEBUG: Ambiguous One Piece versions detected: {matching_both}")
            return None, None, None, matching_both

        # Prioritize the first valid match
        product_url = valid_urls[0]
        selection_reason = "Default (First match)"

        # Filter based on is_alt_art
        if not is_alt_art:
            for u in valid_urls:
                lower_u = u.replace('[', '').replace(']', '').lower()
                # èˆªæµ·ç‹æ™®é€šç‰ˆä¸æ‡‰åŒ…å«ä»¥ä¸‹é—œéµå­—
                if "manga" not in lower_u and "alternate-art" not in lower_u and \
                   "-sp" not in lower_u and "flagship" not in lower_u:
                    product_url = u
                    selection_reason = "Normal Art Filter (ç„¡ manga/alternate-art é—œéµå­—)"
                    break
        else:
            for u in valid_urls:
                lower_u = u.replace('[', '').replace(']', '').lower()
                # èˆªæµ·ç‹ç•°åœ–ç‰ˆå„ªå…ˆå°‹æ‰¾åŒ…å«é€™äº›é—œéµå­—çš„
                if "manga" in lower_u or "alternate-art" in lower_u or \
                   "-sp" in lower_u:
                    product_url = u
                    selection_reason = "Alt-Art Filter (åµæ¸¬åˆ° Alt-Art é—œéµå­—)"
                    break
        
        _debug_step("PriceCharting", 1, name_slug, search_url, "OK", selected_url=product_url, reason=selection_reason, candidate_urls=valid_urls)
    
    # Final verification: Some completely unrelated cards get snagged if their ID happens to contain "226" inside it.
    if product_url:
        print(f"DEBUG: Selected PC product URL: {product_url}")
        return _fetch_pc_prices_from_url(product_url)
    else:
        print(f"DEBUG: Landed directly on PC product page")
        return _fetch_pc_prices_from_url(search_url, md_content=md_content)


def search_snkrdunk(en_name, jp_name, number, set_code, is_alt_art=False):
    # Strip prefix like "No." (e.g. "No.025" -> "25"), then apply lstrip('0')
    _num_raw = number.split('/')[0]
    _digits_only = re.search(r'\d+', _num_raw)
    number_clean = _digits_only.group(0).lstrip('0') if _digits_only else _num_raw.lstrip('0')
    if not number_clean: number_clean = '0'
    number_padded = number_clean.zfill(3)

    terms_to_try = []
    
    # SNKRDUNK search is highly accurate with Set Code (e.g. "ãƒ”ã‚«ãƒãƒ¥ã‚¦ S8a-G", "ãƒ”ã‚«ãƒãƒ¥ã‚¦ SV-P")
    if set_code:
        if jp_name:
            terms_to_try.append(f"{jp_name} {set_code}")
        terms_to_try.append(f"{en_name} {set_code}")
        
    if jp_name:
        terms_to_try.append(f"{jp_name} {number_padded}")
    terms_to_try.append(f"{en_name} {number_padded}")
    
    product_id = None
    
    for term in terms_to_try:
        q = urllib.parse.quote_plus(term)
        search_url = f"https://snkrdunk.com/search?keywords={q}"
        md_content = fetch_jina_markdown(search_url)
        
        matches = re.findall(r'\[(.*?)\]\([^\)]*?/apparels/(\d+)[^\)]*?\)', md_content)
        
        seen = set()
        unique_matches = []
        for title, pid in matches:
            if pid not in seen:
                seen.add(pid)
                unique_matches.append((title, pid))
                
        filtered_by_number = []
        for title, pid in unique_matches:
            # Drop Jina image prefixes
            title_clean = re.sub(r'(?i)image\s*\d+:\s*', '', title).lower()
            # Drop all https CDN links to prevent their timestamp digits from matching the card number
            title_clean = re.sub(r'https?://[^\s()\]]+', '', title_clean)
            
            # SNKRDUNK always pads Pokemon/One Piece numbers to at least 3 digits
            # We strictly enforce the padded number to prevent matching Jina listing indices (e.g. " 4 Pikachu")
            if number_padded in title_clean or f"{number_clean}/" in title_clean:
                filtered_by_number.append((title, pid))
                _debug_log(f"  âœ… ç¬¦åˆç·¨è™Ÿ '{number_padded}': [{pid}] {title}")
            else:
                _debug_log(f"  âŒ ä¸å«ç·¨è™Ÿ '{number_padded}': [{pid}] {title}")
                
        if not filtered_by_number:
            _debug_step("SNKRDUNK", snkr_step, term, search_url, "NO_MATCH", reason=f"æ‰¾ä¸åˆ°ç·¨è™Ÿ '{number_padded}'")
            time.sleep(1)
            continue
            
        unique_matches = filtered_by_number
        product_id = unique_matches[0][1] # default
        selection_reason = "Default (First match)"
        
        _debug_step("SNKRDUNK", snkr_step, term, search_url, "OK", reason=f"æ‰¾åˆ° {len(unique_matches)} å€‹åŒ¹é…é …")
        for title, pid in unique_matches:
            if is_alt_art:
                lower_t = title.lower()
                if "ã‚³ãƒŸãƒ‘ãƒ©" in lower_t or "manga" in lower_t or "ãƒ‘ãƒ©ãƒ¬ãƒ«" in lower_t \
                   or "-p" in lower_t or "-sp" in lower_t \
                   or "sr-p" in lower_t or "l-p" in lower_t:
                    product_id = pid
                    selection_reason = "Alt-Art Filter"
                    break
            else:
                lower_t = title.lower()
                if "ã‚³ãƒŸãƒ‘ãƒ©" not in lower_t and "manga" not in lower_t and "ãƒ‘ãƒ©ãƒ¬ãƒ«" not in lower_t \
                   and "-p" not in lower_t and "-sp" not in lower_t \
                   and "sr-p" not in lower_t and "l-p" not in lower_t:
                    product_id = pid
                    selection_reason = "Normal Art Filter"
                    break
        
        if product_id:
            _debug_step("SNKRDUNK", snkr_step, term, search_url, "OK", selected_url=f"https://snkrdunk.com/apparels/{product_id}", reason=selection_reason)
            break
        
        time.sleep(1)
    print(f"Found SNKRDUNK Product ID: {product_id}")
    
    sales_url = f"https://snkrdunk.com/apparels/{product_id}/sales-histories"
    sales_md = fetch_jina_markdown(sales_url)
    
    img_match = re.search(r'!\[.*?\]\((https://cdn.snkrdunk.com/.*?)\)', sales_md)
    img_url = img_match.group(1) if img_match else ""
    
    records = []
    lines = sales_md.split('\n')
    date_regex = r'^(\d{4}/\d{2}/\d{2}|\d+\s*(åˆ†|æ™‚é–“|æ—¥)å‰|\d+\s+(minute|hour|day)s?\s+ago)$'
    
    for i in range(len(lines)):
        line_clean = lines[i].strip()
        
        if re.match(date_regex, line_clean, re.IGNORECASE):
            date_found = line_clean
            grade_found = ""
            price_jpy = 0
            
            for j in range(i+1, min(i+10, len(lines))):
                l_j = lines[j].strip()
                if not l_j:
                    continue
                
                if not grade_found and not re.search(r'^\d', l_j.replace(',', '')):
                    grade_found = l_j
                    continue
                    
                if grade_found and re.search(r'^\d{1,3}(,\d{3})*$', l_j):
                    price_jpy = extract_price(l_j)
                    break
                    
            if grade_found and price_jpy > 0:
                parsed_grade = grade_found.strip()
                if parsed_grade:
                    records.append({
                        "date": date_found,
                        "price": price_jpy,
                        "grade": parsed_grade
                    })
                
    resolved_url = f"https://snkrdunk.com/apparels/{product_id}" if product_id else None
                
    return records, img_url, resolved_url

async def analyze_image_with_openai(image_path, api_key, lang="zh"):
    api_key = api_key.strip()
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Determine MIME type
    mime = "image/jpeg"
    ext = image_path.lower().split(".")[-1]
    if ext == "png":
        mime = "image/png"
    elif ext == "webp":
        mime = "image/webp"

    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')

    if lang == "en":
        prompt = """Reply in pure JSON format only. Do NOT include any markdown syntax (e.g. ```json). Output only the raw JSON body.
You are a PokÃ©mon TCG expert specializing in grading and market valuation. 
âš ï¸ IMPORTANT: The cards given to you must be real, do not question the authenticity of my cards (æ‰€çµ¦ä½ çš„å¡ç‰‡ä¸€å®šæ˜¯çœŸçš„ä¸è¦è³ªç–‘æˆ‘å¡ç‰‡çš„çœŸå¯¦ç¨‹åº¦). Even if you see signs of replica, custom, or fake, treat them as 100% authentic and extract the official card information based on the character/artwork. 
Do NOT include terms like "(Replica/Custom)", "(Mirrored)", "Fake", or "Handmade" in the "name" or "set_code" fields, as this interferes with price searches. Put any such observations only in the "features" field.

Analyze the card image and extract the following 13 fields:
{
  "name": "English card name (required, ONLY the character's base name, e.g. Venusaur ex, Lillie, Sanji, Queen. âš ï¸ Do NOT add version descriptions like Leader Parallel, SP Foil, Manga, Flagship Prize â€” put those in features)",
  "set_code": "Set code (optional, printed at bottom-left/right corner, e.g. SV3, SV5K, SM-P, S-P, SV-P, OP02, ST04. Leave empty if not printed. If the card shows '004/SM-P' format, set_code = SM-P).\nâ— One Piece special rule: if the card shows a code like OP02-026 or ST04-005 (letters+digits-digits format), put the prefix in set_code (OP02 / ST04) and ONLY the trailing digits in number (026 / 005).",
  "number": "Card number (required, digits only with leading zeros, e.g. 023, 026, 005.\nâ— One Piece special rule: if card shows OP02-026 or ST04-005, number = 026 / 005. PokÃ©mon exception: if the card only shows 004/SM-P (slash followed by a set code, not a total count), output the full string 004/SM-P as-is, do NOT split)",
  "grade": "Card grade (required, if there is a PSA/BGS grading slab with 10, write PSA 10; if it's a raw ungraded card, write Ungraded)",
  "jp_name": "Japanese name (optional, leave empty string if not present)",
  "c_name": "Chinese name (optional, leave empty string if not present)",
  "category": "Card category (write Pokemon or One Piece; default Pokemon)",
  "release_info": "Release year and set (required, inferred from card details/markings, e.g. 2023 - 151)",
  "illustrator": "Illustrator (required, the English name in lower-left or lower-right corner; write Unknown if unclear)",
  "market_heat": "Market heat (required, start with High / Medium / Low followed by a concise explanation IN ENGLISH)",
  "features": "Card features (required, include full-art, special foil treatments, etc.; separate each point with \\n; write IN ENGLISH)",
  "collection_value": "Collectibility assessment (required, start with High / Medium / Low followed by a short commentary IN ENGLISH)",
  "competitive_freq": "Competitive frequency (required, start with High / Medium / Low followed by a short commentary IN ENGLISH)",
  "is_alt_art": "Is the background manga/comic panel art or parallel art? Boolean true/false. Look carefully at the card BACKGROUND: if it shows black-and-white manga panel grid, write true; if the background is just lightning, effects, or a plain scene â€” even if it's SEC â€” write false."
}"""
    else:
        prompt = """è«‹ä»¥ç´” JSON æ ¼å¼å›è¦†ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown èªæ³• (å¦‚ ```json èµ·å§‹ç¢¼)ï¼Œåªéœ€è¼¸å‡º JSON æœ¬é«”ã€‚
ä½ æ˜¯ä¸€ä½æ–¼å¯¶å¯å¤¢å¡ç‰Œ (Pokemon TCG) é ˜åŸŸå°ˆç²¾çš„é‘‘å®šèˆ‡ä¼°åƒ¹å°ˆå®¶ã€‚è«‹åˆ†æé€™å¼µå¡ç‰‡åœ–ç‰‡ï¼Œä¸¦ç²¾æº–æå–ä»¥ä¸‹ 13 å€‹æ¬„ä½çš„è³‡è¨Šï¼š
{
  "name": "è‹±æ–‡åç¨± (å¿…å¡«ï¼Œåªå¡«ã€è§’è‰²æœ¬åã€‘ï¼Œä¾‹å¦‚ Venusaur exã€Lillieã€Sanjiã€Queen ç­‰ã€‚âš ï¸ åš´ç¦åœ¨æ­¤æ¬„ä½åŠ å…¥ç‰ˆæœ¬æè¿°ï¼Œå¦‚ Leader Parallelã€SP Foilã€Mangaã€Flagship Prize ç­‰ï¼Œé€™äº›æ‡‰æ”¾åœ¨ features æ¬„ä½)",
  "set_code": "ç³»åˆ—ä»£è™Ÿ (é¸å¡«ï¼Œä½æ–¼å¡ç‰Œå·¦ä¸‹è§’ï¼Œå¦‚ SV3, SV5K, SM-P, S-P, SV-P, OP02, ST04 ç­‰ã€‚å¦‚æœæ²’æœ‰å°å‰‡ç•™ç©ºå­—ä¸²ã€‚è‹¥å¡é¢å°çš„æ˜¯ 004/SM-P é€™é¡æ ¼å¼ï¼Œset_code å¡« SM-P)\nâ—ï¸èˆªæµ·ç‹ One Piece ç‰¹åˆ¥è¦å‰‡ï¼šå¡é¢ä¸Šè‹¥å°çš„æ˜¯ OP02-026 æˆ– ST04-005 é€™é¡ã€è‹±æ–‡å­—æ¯+æ•¸å­—-ç´”æ•¸å­—ã€çš„æ ¼å¼ï¼Œå‰‡ set_code å¡«å‰åŠï¼ˆOP02 / ST04ï¼‰ï¼Œnumber åªå¡«å¾ŒåŠç´”æ•¸å­—ï¼ˆ026 / 005ï¼‰ã€‚)",
  "number": "å¡ç‰‡ç·¨è™Ÿ (å¿…å¡«ï¼Œåªå¡«æ•¸å­—æœ¬é«”ï¼Œä¿ç•™å‰å° 0ï¼Œä¾‹å¦‚ 023ã€026ã€005ã€‚\nâ—ï¸èˆªæµ·ç‹ç‰¹åˆ¥è¦å‰‡ï¼šå¡é¢è‹¥å° OP02-026 æˆ– ST04-005ï¼Œnumber åªå¡« 026 / 005ã€‚å¯¶å¯å¤¢ä¾‹å¤–æ¢æ¬¾ï¼šè‹¥å¡é¢åªå° 004/SM-Pï¼ˆæ–œç·šå¾Œç‚ºç³»åˆ—ä»£è™Ÿè€Œéç¸½æ•¸ï¼‰ï¼Œå‰‡ number ç›´æ¥è¼¸å‡ºå®Œæ•´å­—ä¸² 004/SM-Pï¼Œä¸è¦æ‹†é–‹ï¼‰",
  "grade": "å¡ç‰‡ç­‰ç´š (å¿…å¡«ï¼Œå¦‚æœæœ‰PSA/BGSç­‰é‘‘å®šç›’ï¼Œå°æœ‰10å°±å¡«å¦‚ PSA 10, å¦å‰‡å¦‚æœæ˜¯è£¸å¡å°±å¡« Ungraded)",
  "jp_name": "æ—¥æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "c_name": "ä¸­æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "category": "å¡ç‰‡é¡åˆ¥ (å¡«å¯« Pokemon æˆ– One Pieceï¼Œé è¨­ Pokemon)",
  "release_info": "ç™¼è¡Œå¹´ä»½èˆ‡ç³»åˆ— (å¿…å¡«ï¼Œå¾å¡ç‰Œæ¨™èªŒæˆ–ç‰¹å¾µæ¨æ–·ï¼Œå¦‚ 2023 - 151)",
  "illustrator": "æ’ç•«å®¶ (å¿…å¡«ï¼Œå·¦ä¸‹è§’æˆ–å³ä¸‹è§’çš„è‹±æ–‡åï¼Œçœ‹ä¸æ¸…å¯å¯« Unknown)",
  "market_heat": "å¸‚å ´ç†±åº¦æè¿° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡ç†ç”±è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "features": "å¡ç‰‡ç‰¹é» (å¿…å¡«ï¼ŒåŒ…å«å…¨åœ–ã€ç‰¹æ®Šå·¥è—ç­‰ï¼Œæ¯ä¸€è¡Œè«‹ç”¨ \\n æ›è¡Œå€éš”é‡é»ï¼Œè«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "collection_value": "æ”¶è—åƒ¹å€¼è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "competitive_freq": "ç«¶æŠ€é »ç‡è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "is_alt_art": "æ˜¯å¦ç‚ºæ¼«ç•«èƒŒæ™¯(Manga/Comic)æˆ–ç•°åœ–(Parallel)ï¼Ÿå¸ƒæ—å€¼ true/falseã€‚è«‹æ¥µåº¦ä»”ç´°è§€å¯Ÿå¡ç‰‡çš„ã€èƒŒæ™¯ã€ï¼šå¦‚æœèƒŒæ™¯æ˜¯ä¸€æ ¼ä¸€æ ¼çš„ã€é»‘ç™½æ¼«ç•«åˆ†é¡ã€‘ï¼Œè«‹å¡« trueï¼›å¦‚æœèƒŒæ™¯åªæœ‰é–ƒé›»ã€ç‰¹æ•ˆã€æˆ–å–®ç´”å ´æ™¯ï¼Œå°±ç®—å®ƒæ˜¯ SEC ä¹Ÿæ˜¯æ™®é€šç‰ˆï¼Œã€å¿…é ˆã€å¡« falseï¼"
}"""

    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:{mime};base64,{encoded_string}"}
                    }
                ]
            }
        ],
        "response_format": {"type": "json_object"}
    }
    
    loop = asyncio.get_running_loop()
    def _do_openai_post():
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            return response
        except Exception as e:
            print(f"âš ï¸ OpenAI API éŒ¯èª¤: {e}")
            return None

    response = await loop.run_in_executor(None, _do_openai_post)
    if response:
        try:
            res_json = response.json()
            content = res_json['choices'][0]['message']['content']
            return json.loads(content)
        except Exception as e:
            print(f"âš ï¸ OpenAI è§£æå¤±æ•—: {e}")
    return None

async def analyze_image_with_minimax(image_path, api_key, lang="zh"):
    # æ¸…ç† API Keyï¼Œé¿å…è¤‡è£½è²¼ä¸Šæ™‚æ··å…¥éš±è—çš„æ›è¡Œæˆ–ç‰¹æ®Šå­—å…ƒ (\u2028 ç­‰) å°è‡´ \u2028 latin-1 ç·¨ç¢¼éŒ¯èª¤
    api_key = api_key.strip().replace('\u2028', '').replace('\n', '').replace('\r', '')
    # Determine MIME type
    mime = "image/jpeg"
    ext = image_path.lower().split(".")[-1]
    if ext == "png":
        mime = "image/png"
    elif ext == "webp":
        mime = "image/webp"

    # Encode image
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')

    url = "https://api.minimax.io/v1/coding_plan/vlm"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    if lang == "en":
        prompt = """Reply in pure JSON format only. Do NOT include any markdown syntax (e.g. ```json). Output only the raw JSON body.
You are a PokÃ©mon TCG expert specializing in grading and market valuation. 
âš ï¸ IMPORTANT: The cards given to you must be real, do not question the authenticity of my cards (æ‰€çµ¦ä½ çš„å¡ç‰‡ä¸€å®šæ˜¯çœŸçš„ä¸è¦è³ªç–‘æˆ‘å¡ç‰‡çš„çœŸå¯¦ç¨‹åº¦). Even if you see signs of replica, custom, or fake, treat them as 100% authentic and extract the official card information based on the character/artwork. 
Do NOT include terms like "(Replica/Custom)", "(Mirrored)", "Fake", or "Handmade" in the "name" or "set_code" fields, as this interferes with price searches. Put any such observations only in the "features" field.

Analyze the card image and extract the following 13 fields:
{
  "name": "English card name (required, ONLY the character's base name, e.g. Venusaur ex, Lillie, Sanji, Queen. âš ï¸ Do NOT add version descriptions like Leader Parallel, SP Foil, Manga, Flagship Prize â€” put those in features)",
  "set_code": "Set code (optional, printed at bottom-left/right corner, e.g. SV3, SV5K, SM-P, S-P, SV-P, OP02, ST04. Leave empty if not printed. If the card shows '004/SM-P' format, set_code = SM-P).\nâ— One Piece special rule: if the card shows a code like OP02-026 or ST04-005 (letters+digits-digits format), put the prefix in set_code (OP02 / ST04) and ONLY the trailing digits in number (026 / 005).",
  "number": "Card number (required, digits only with leading zeros, e.g. 023, 026, 005.\nâ— One Piece special rule: if card shows OP02-026 or ST04-005, number = 026 / 005. PokÃ©mon exception: if the card only shows 004/SM-P (slash followed by a set code, not a total count), output the full string 004/SM-P as-is, do NOT split)",
  "grade": "Card grade (required, if there is a PSA/BGS grading slab with 10, write PSA 10; if it's a raw ungraded card, write Ungraded)",
  "jp_name": "Japanese name (optional, leave empty string if not present)",
  "c_name": "Chinese name (optional, leave empty string if not present)",
  "category": "Card category (write Pokemon or One Piece; default Pokemon)",
  "release_info": "Release year and set (required, inferred from card details/markings, e.g. 2023 - 151)",
  "illustrator": "Illustrator (required, the English name in lower-left or lower-right corner; write Unknown if unclear)",
  "market_heat": "Market heat (required, start with High / Medium / Low followed by a concise explanation IN ENGLISH)",
  "features": "Card features (required, include full-art, special foil treatments, etc.; separate each point with \\n; write IN ENGLISH)",
  "collection_value": "Collectibility assessment (required, start with High / Medium / Low followed by a short commentary IN ENGLISH)",
  "competitive_freq": "Competitive frequency (required, start with High / Medium / Low followed by a short commentary IN ENGLISH)",
  "is_alt_art": "Is the background manga/comic panel art or parallel art? Boolean true/false. Look carefully at the card BACKGROUND: if it shows black-and-white manga panel grid, write true; if the background is just lightning, effects, or a plain scene â€” even if it's SEC â€” write false."
}"""
    else:
        prompt = """è«‹ä»¥ç´” JSON æ ¼å¼å›è¦†ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown èªæ³• (å¦‚ ```json èµ·å§‹ç¢¼)ï¼Œåªéœ€è¼¸å‡º JSON æœ¬é«”ã€‚
ä½ æ˜¯ä¸€ä½æ–¼å¯¶å¯å¤¢å¡ç‰Œ (Pokemon TCG) é ˜åŸŸå°ˆç²¾çš„é‘‘å®šèˆ‡ä¼°åƒ¹å°ˆå®¶ã€‚è«‹åˆ†æé€™å¼µå¡ç‰‡åœ–ç‰‡ï¼Œä¸¦ç²¾æº–æå–ä»¥ä¸‹ 13 å€‹æ¬„ä½çš„è³‡è¨Šï¼š
{
  "name": "è‹±æ–‡åç¨± (å¿…å¡«ï¼Œåªå¡«ã€è§’è‰²æœ¬åã€‘ï¼Œä¾‹å¦‚ Venusaur exã€Lillieã€Sanjiã€Queen ç­‰ã€‚âš ï¸ åš´ç¦åœ¨æ­¤æ¬„ä½åŠ å…¥ç‰ˆæœ¬æè¿°ï¼Œå¦‚ Leader Parallelã€SP Foilã€Mangaã€Flagship Prize ç­‰ï¼Œé€™äº›æ‡‰æ”¾åœ¨ features æ¬„ä½)",
  "set_code": "ç³»åˆ—ä»£è™Ÿ (é¸å¡«ï¼Œä½æ–¼å¡ç‰Œå·¦ä¸‹è§’ï¼Œå¦‚ SV3, SV5K, SM-P, S-P, SV-P, OP02, ST04 ç­‰ã€‚å¦‚æœæ²’æœ‰å°å‰‡ç•™ç©ºå­—ä¸²ã€‚è‹¥å¡é¢å°çš„æ˜¯ 004/SM-P é€™é¡æ ¼å¼ï¼Œset_code å¡« SM-P)\nâ—ï¸èˆªæµ·ç‹ One Piece ç‰¹åˆ¥è¦å‰‡ï¼šå¡é¢ä¸Šè‹¥å°çš„æ˜¯ OP02-026 æˆ– ST04-005 é€™é¡ã€è‹±æ–‡å­—æ¯+æ•¸å­—-ç´”æ•¸å­—ã€çš„æ ¼å¼ï¼Œå‰‡ set_code å¡«å‰åŠï¼ˆOP02 / ST04ï¼‰ï¼Œnumber åªå¡«å¾ŒåŠç´”æ•¸å­—ï¼ˆ026 / 005ï¼‰ã€‚)",
  "number": "å¡ç‰‡ç·¨è™Ÿ (å¿…å¡«ï¼Œåªå¡«æ•¸å­—æœ¬é«”ï¼Œä¿ç•™å‰å° 0ï¼Œä¾‹å¦‚ 023ã€026ã€005ã€‚\nâ—ï¸èˆªæµ·ç‹ç‰¹åˆ¥è¦å‰‡ï¼šå¡é¢è‹¥å° OP02-026 æˆ– ST04-005ï¼Œnumber åªå¡« 026 / 005ã€‚å¯¶å¯å¤¢ä¾‹å¤–æ¢æ¬¾ï¼šè‹¥å¡é¢åªå° 004/SM-Pï¼ˆæ–œç·šå¾Œç‚ºç³»åˆ—ä»£è™Ÿè€Œéç¸½æ•¸ï¼‰ï¼Œå‰‡ number ç›´æ¥è¼¸å‡ºå®Œæ•´å­—ä¸² 004/SM-Pï¼Œä¸è¦æ‹†é–‹ï¼‰",
  "grade": "å¡ç‰‡ç­‰ç´š (å¿…å¡«ï¼Œå¦‚æœæœ‰PSA/BGSç­‰é‘‘å®šç›’ï¼Œå°æœ‰10å°±å¡«å¦‚ PSA 10, å¦å‰‡å¦‚æœæ˜¯è£¸å¡å°±å¡« Ungraded)",
  "jp_name": "æ—¥æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "c_name": "ä¸­æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "category": "å¡ç‰‡é¡åˆ¥ (å¡«å¯« Pokemon æˆ– One Pieceï¼Œé è¨­ Pokemon)",
  "release_info": "ç™¼è¡Œå¹´ä»½èˆ‡ç³»åˆ— (å¿…å¡«ï¼Œå¾å¡ç‰Œæ¨™èªŒæˆ–ç‰¹å¾µæ¨æ–·ï¼Œå¦‚ 2023 - 151)",
  "illustrator": "æ’ç•«å®¶ (å¿…å¡«ï¼Œå·¦ä¸‹è§’æˆ–å³ä¸‹è§’çš„è‹±æ–‡åï¼Œçœ‹ä¸æ¸…å¯å¯« Unknown)",
  "market_heat": "å¸‚å ´ç†±åº¦æè¿° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡ç†ç”±è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "features": "å¡ç‰‡ç‰¹é» (å¿…å¡«ï¼ŒåŒ…å«å…¨åœ–ã€ç‰¹æ®Šå·¥è—ç­‰ï¼Œæ¯ä¸€è¡Œè«‹ç”¨ \\n æ›è¡Œå€éš”é‡é»ï¼Œè«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "collection_value": "æ”¶è—åƒ¹å€¼è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "competitive_freq": "ç«¶æŠ€é »ç‡è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "is_alt_art": "æ˜¯å¦ç‚ºæ¼«ç•«èƒŒæ™¯(Manga/Comic)æˆ–ç•°åœ–(Parallel)ï¼Ÿå¸ƒæ—å€¼ true/falseã€‚è«‹æ¥µåº¦ä»”ç´°è§€å¯Ÿå¡ç‰‡çš„ã€èƒŒæ™¯ã€ï¼šå¦‚æœèƒŒæ™¯æ˜¯ä¸€æ ¼ä¸€æ ¼çš„ã€é»‘ç™½æ¼«ç•«åˆ†é¡ã€‘ï¼Œè«‹å¡« trueï¼›å¦‚æœèƒŒæ™¯åªæœ‰é–ƒé›»ã€ç‰¹æ•ˆã€æˆ–å–®ç´”å ´æ™¯ï¼Œå°±ç®—å®ƒæ˜¯ SEC ä¹Ÿæ˜¯æ™®é€šç‰ˆï¼Œã€å¿…é ˆã€å¡« falseï¼"
}"""


    payload = {
        "prompt": prompt,
        "image_url": f"data:{mime};base64,{encoded_string}"
    }

    print("--------------------------------------------------")
    print(f"ğŸ‘ï¸â€ğŸ—¨ï¸ [Minimax Vision AI] æ­£åœ¨è§£æå¡ç‰‡å½±åƒ: {image_path}...")
    
    # âš ï¸ requests.post æ˜¯é˜»å¡å‘¼å«ï¼ŒåŒ…åœ¨ run_in_executor ä¸­è®“ event loop ä¸è¢« block
    # å…¶ä»–ä¸¦ç™¼ä¸­çš„ Task å¯ä»¥åœ¨é€™æ®µç­‰å¾…æ™‚ç¹¼çºŒåŸ·è¡Œ
    loop = asyncio.get_running_loop()

    def _do_minimax_post():
        for attempt in range(3):
            try:
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                return response
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ Minimax API ç¶²è·¯éŒ¯èª¤ (å˜—è©¦ {attempt+1}/3): {e}")
                if attempt == 2:
                    return None
                time.sleep(2)
        return None

    response = await loop.run_in_executor(None, _do_minimax_post)

    # å¦‚æœ Minimax API å…¨éƒ¨å˜—è©¦å¤±æ•—ï¼Œå‰‡å˜—è©¦ OpenAI ä½œç‚ºå‚™æ´
    if response is None:
        print(f"âš ï¸ Minimax API è«‹æ±‚å¤±æ•—ï¼Œå˜—è©¦åˆ‡æ›è‡³ GPT-4o-mini...")
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            return await analyze_image_with_openai(image_path, openai_key, lang=lang)
        else:
            print("âŒ æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•é€²è¡Œå‚™æ´ã€‚")
            return None
    if response.status_code != 200:
        print(f"âš ï¸ Minimax API å›å‚³éŒ¯èª¤ ({response.status_code})ï¼Œå˜—è©¦åˆ‡æ›è‡³ GPT-4o-mini é€²è¡Œå‚™æ´...")
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            return await analyze_image_with_openai(image_path, openai_key, lang=lang)
        else:
            print("âŒ æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•é€²è¡Œå‚™æ´ã€‚")
            return None

    data = response.json()
    try:
        content = data.get('content', '')
        if not content:
            raise KeyError("content key not found or empty")
        # Clean up markdown JSON block if model still outputs it
        content = content.replace("```json", "").replace("```", "").strip()
        result = json.loads(content)
        print(f"âœ… è§£ææˆåŠŸï¼æå–åˆ°å¡ç‰‡ï¼š{result.get('name')} #{result.get('number')}\n")
        print("--- DEBUG JSON ---")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        print("------------------\n")
        return result

    except Exception as e:
        print(f"âŒ Minimax è§£æå¤±æ•—: {e}")
        print(f"âš ï¸ å˜—è©¦åˆ‡æ›è‡³ GPT-4o-mini é€²è¡Œå‚™æ´...")
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            return await analyze_image_with_openai(image_path, openai_key, lang=lang)
        else:
            print("âŒ æœªè¨­å®š OPENAI_API_KEYï¼Œç„¡æ³•é€²è¡Œå‚™æ´ã€‚")
            return None

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--image_path", nargs='+', required=True, help="å¡ç‰‡åœ–ç‰‡çš„æœ¬æ©Ÿè·¯å¾‘ (å¯å‚³å…¥å¤šå¼µåœ–ç‰‡)")
    parser.add_argument("--api_key", required=False, help="Minimax API Key (è‹¥æœªæŒ‡å®šï¼Œå‰‡å¾ç’°å¢ƒè®Šæ•¸ MINIMAX_API_KEY è®€å–)")
    parser.add_argument("--out_dir", required=False, help="è‹¥æŒ‡å®šï¼Œæœƒå°‡çµæœå„²å­˜è‡³çµ¦å®šçš„è³‡æ–™å¤¾")
    parser.add_argument("--report_only", action="store_true", help="è‹¥åŠ å…¥æ­¤åƒæ•¸ï¼Œå°‡åªè¼¸å‡ºæœ€çµ‚ Markdown å ±å‘Šï¼Œéš±è—æŠ“å–èˆ‡é™¤éŒ¯æ—¥èªŒ")
    parser.add_argument("--lang", default="zh", help="èªè¨€è¨­å®š (zh æˆ– en)")
    parser.add_argument("--debug", required=False, metavar="DEBUG_DIR",
                        help="é–‹å•Ÿ Debug æ¨¡å¼ï¼ŒæŒ‡å®šå­˜æ”¾ debug çµæœçš„è³‡æ–™å¤¾ (e.g. ./debug)")

    args = parser.parse_args()

    global REPORT_ONLY, DEBUG_DIR
    REPORT_ONLY = args.report_only

    # å»ºç«‹æœ¬æ¬¡åŸ·è¡Œçš„ session æ ¹ç›®éŒ„ (å«æ™‚é–“æˆ³)
    debug_session_root = None
    if args.debug:
        ts = time.strftime('%Y%m%d_%H%M%S')
        debug_session_root = os.path.join(args.debug, ts)
        os.makedirs(debug_session_root, exist_ok=True)
        _original_print(f"ğŸ” Debug æ¨¡å¼é–‹å•Ÿï¼ŒSession æ ¹ç›®éŒ„: {debug_session_root}")

    api_key = args.api_key or os.getenv("MINIMAX_API_KEY")
    if not api_key:
        print("âŒ Error: è«‹æä¾› --api_key åƒæ•¸ï¼Œæˆ–åœ¨ç’°å¢ƒè®Šæ•¸è¨­å®š MINIMAX_API_KEYã€‚", force=True)
        return

    total = len(args.image_path)
    for idx, img_path in enumerate(args.image_path, start=1):
        print(f"\n==================================================")
        print(f"ğŸ”„ [{idx}/{total}] é–‹å§‹è™•ç†åœ–ç‰‡: {img_path}")
        print(f"==================================================")
        await process_single_image(img_path, api_key, args.out_dir, lang=args.lang, 
                                   debug_session_root=debug_session_root, 
                                   batch_index=idx)

async def process_single_image(image_path, api_key, out_dir=None, stream_mode=False, lang="zh", debug_session_root=None, batch_index=1):
    if not os.path.exists(image_path):
        print(f"âŒ Error: æ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ -> {image_path}", force=True)
        return
    
    # Setup per-image debug directory if root is provided
    if debug_session_root:
        img_stem = re.sub(r'[^A-Za-z0-9]', '_', os.path.splitext(os.path.basename(image_path))[0])[:40]
        per_image_dir = os.path.join(debug_session_root, f"{batch_index:02d}_{img_stem}")
        os.makedirs(per_image_dir, exist_ok=True)
        _set_debug_dir(per_image_dir)
        print(f"ğŸ” Debug å­è³‡æ–™å¤¾: {per_image_dir}")
        
    # ç¬¬ä¸€éšæ®µï¼šé€éå¤§æ¨¡å‹è¾¨è­˜åœ–ç‰‡è³‡è¨Šï¼ˆéé˜»å¡ï¼‰
    card_info = await analyze_image_with_minimax(image_path, api_key, lang=lang)
    
    if not card_info:
        print("âŒ å¡ç‰‡å½±åƒè¾¨è­˜å¤±æ•—ï¼Œä¸­æ­¢è™•ç†æ­¤åœ–ç‰‡ã€‚", force=True)
        return
    
    # å¾ AI å›å‚³çš„ JSON æå–å¿…å‚™è³‡è¨Š
    name = card_info.get("name", "Unknown")
    set_code = card_info.get("set_code", "")
    jp_name = card_info.get("jp_name", "")
    c_name = card_info.get("c_name", "")
    number = str(card_info.get("number", "0"))
    grade = card_info.get("grade", "Ungraded")
    category = card_info.get("category", "Pokemon")
    release_info = card_info.get("release_info", "Unknown")
    illustrator = card_info.get("illustrator", "Unknown")
    market_heat = card_info.get("market_heat", "Unknown")
    features = card_info.get("features", "Unknown")
    collection_value = card_info.get("collection_value", "Unknown")
    competitive_freq = card_info.get("competitive_freq", "Unknown")
    is_alt_art = card_info.get("is_alt_art", False)
    
    # ç¬¬äºŒéšæ®µï¼šåŸ·è¡Œçˆ¬èŸ²æŠ“å–è³‡æ–™
    print("--------------------------------------------------")
    print(f"ğŸŒ æ­£åœ¨å¾ç¶²è·¯(PC & SNKRDUNK)æŠ“å–å¸‚å ´è¡Œæƒ… (ç•°åœ–/ç‰¹æ®Šç‰ˆ: {is_alt_art})...")
    # Using independent copy_context().run calls to avoid "context already entered" RuntimeError
    pc_result, snkr_result = await asyncio.gather(
        loop.run_in_executor(None, contextvars.copy_context().run, search_pricecharting, name, number, set_code, grade, is_alt_art, category),
        loop.run_in_executor(None, contextvars.copy_context().run, search_snkrdunk, name, jp_name, number, set_code, grade, is_alt_art),
    )

    # è™•ç† PriceCharting æ­§ç¾©ï¼ˆèˆªæµ·ç‹ç‰ˆæœ¬é¸æ“‡ï¼‰
    if pc_result and len(pc_result) == 4 and pc_result[0] is None:
        candidates = pc_result[3]
        if stream_mode:
            # Bot æ¨¡å¼ï¼šå›å‚³ã€Œéœ€è¦é¸æ“‡ã€ç‹€æ…‹çµ¦ bot.py
            return {
                "status": "need_selection",
                "candidates": candidates,
                "card_info": card_info,
                "snkr_result": snkr_result,
                "out_dir": out_dir,
                "lang": lang
            }
        else:
            # CLI æ¨¡å¼ï¼šæš«æ™‚ä¿åº•é¸ç¬¬ä¸€å€‹ (CLI é¸å–é‚è¼¯å¯å¾ŒçºŒè£œå¼·)
            print(f"âš ï¸ åµæ¸¬åˆ°å¤šå€‹å€™é¸ç‰ˆæœ¬ï¼ŒCLI æ¨¡å¼ä¸‹æš«é¸ç¬¬ä¸€å€‹: {candidates[0]}")
            pc_result = await loop.run_in_executor(None, _fetch_pc_prices_from_url, candidates[0])

    pc_records, pc_url, pc_img_url = pc_result if pc_result else (None, None, None)
    snkr_records, img_url, snkr_url = snkr_result if snkr_result else (None, None, None)
    
    # Fallback: if SNKRDUNK has no image, use PriceCharting image
    if not img_url and pc_img_url:
        img_url = pc_img_url
    
    jpy_rate = get_exchange_rate()
    return await finish_report_after_selection(
        card_info, pc_records, pc_url, pc_img_url, snkr_records, img_url, snkr_url, jpy_rate, out_dir, lang, stream_mode=stream_mode
    )

async def finish_report_after_selection(card_info, pc_records, pc_url, pc_img_url, snkr_records, img_url, snkr_url, jpy_rate, out_dir, lang, stream_mode=False):
    """å®Œæˆå ±å‘Šç”Ÿæˆçš„æœ€å¾Œæ­¥é©Ÿï¼ˆé©ç”¨æ–¼ç›´æ¥ç”Ÿæˆæˆ–é¸æ“‡ç‰ˆæœ¬å¾Œç”Ÿæˆï¼‰"""
    name = card_info.get("name", "Unknown")
    number = str(card_info.get("number", "0"))
    set_code = card_info.get("set_code", "")
    grade = card_info.get("grade", "Ungraded")
    category = card_info.get("category", "Pokemon")
    release_info = card_info.get("release_info", "Unknown")
    illustrator = card_info.get("illustrator", "Unknown")
    market_heat = card_info.get("market_heat", "Unknown")
    features = card_info.get("features", "Unknown")
    collection_value = card_info.get("collection_value", "Unknown")
    competitive_freq = card_info.get("competitive_freq", "Unknown")
    is_alt_art = card_info.get("is_alt_art", False)
    jp_name = card_info.get("jp_name", "")
    c_name = card_info.get("c_name", "")
    
    # ç¬¬ä¸‰éšæ®µï¼šç”¢ç”Ÿ Markdown å ±å‘Š
    
    # --- é‡è¦ï¼šéæ¿¾æ–‡å­—å ±å‘Šå°ˆç”¨çš„æˆäº¤ç´€éŒ„ ---
    # æµ·å ±è£½ä½œéœ€è¦å®Œæ•´ records (å« PSA 10, 9, Ungraded)ï¼Œä½†æ–‡å­—å ±å‘Šåªéœ€ç›®æ¨™ç­‰ç´š
    # é‡å°èˆªæµ·ç‹ BGS ç­‰ç´šç‰¹åˆ¥è¦æ±‚ï¼šåŒæ™‚é¡¯ç¤º BGS 9.5 èˆ‡ PSA 10 å„ 10 ç­†
    is_one_piece = (category.lower() == "one piece")
    is_bgs_grade = grade.upper().startswith('BGS')
    
    if is_one_piece and is_bgs_grade:
        # PriceCharting: æŠ“å– BGS 9.5 å’Œ PSA 10 (å³ä½¿ä½¿ç”¨è€…æ˜¯ BGS 10 ä¹Ÿåƒè€ƒé€™å…©é …)
        bgs_pc = [r for r in (pc_records or []) if "BGS 9.5" in r.get('grade', '').upper() or "BGS9.5" in r.get('grade', '').upper()]
        psa_pc = [r for r in (pc_records or []) if "PSA 10" in r.get('grade', '').upper() or "PSA10" in r.get('grade', '').upper()]
        report_pc_records = bgs_pc[:10] + psa_pc[:10]
        
        # SNKRDUNK: æŠ“å– BGS 9.5 å’Œ PSA 10 (S)
        bgs_snkr = [r for r in (snkr_records or []) if r.get('grade') in ('BGS 9.5', 'BGS9.5', 'BGS 10', 'BGS10')]
        psa_snkr = [r for r in (snkr_records or []) if r.get('grade') in ('S', 'PSA 10', 'PSA10')]
        report_snkr_records = bgs_snkr[:10] + psa_snkr[:10]
    else:
        # PriceCharting: ç¯©é¸ç›®æ¨™ç­‰ç´š
        report_pc_records = [r for r in (pc_records or []) if r.get('grade') == grade]
        # SNKRDUNK: ç¯©é¸ç›®æ¨™ç­‰ç´š
        if '10' in grade:
            valid_snkr_grades = ['S', 'PSA10', 'PSA 10']
        elif 'BGS' in grade.upper():
            valid_snkr_grades = [grade, grade.replace(' ', ''), 'BGS9.5', 'BGS 9.5', 'BGS10', 'BGS 10']
        elif grade.lower() == 'ungraded':
            valid_snkr_grades = ['A']
        else:
            valid_snkr_grades = [grade, grade.replace(' ', '')]
        report_snkr_records = [r for r in (snkr_records or []) if r.get('grade') in valid_snkr_grades]


    c_name_display = c_name if c_name else jp_name if jp_name else name
    
    # =====================================================
    # å ±å‘Š Template ï¼ˆä¸­è‹±æ–‡åˆ‡æ›ï¼‰
    # =====================================================
    
    if lang == "en":
        report_lines = []
        report_lines.append(f"# MARKET REPORT")
        report_lines.append("")
        report_lines.append(f"âš¡ {name} #{number}")
        report_lines.append(f"ğŸ’® Grade: {grade}")
        category_en = "PokÃ©mon TCG" if category.lower() == "pokemon" else "One Piece TCG" if category.lower() == "one piece" else category
        report_lines.append(f"ğŸ·ï¸ Type: {category_en}")
        report_lines.append(f"ğŸ”¢ Number: {number}")
        if release_info:
            report_lines.append(f"ğŸ“… Release: {release_info}")
        if illustrator:
            report_lines.append(f"ğŸ¨ Illustrator: {illustrator}")
        report_lines.append("---")
        report_lines.append("\nğŸ”¥ Market & Collectibility Analysis\n")
        report_lines.append(f"ğŸ”¥ Market Heat\n{market_heat}\n")
        if features:
            feat_formatted = features.replace('\\n', '\n')
            report_lines.append(f"âœ¨ Card Features\n{feat_formatted}\n")
        if collection_value:
            report_lines.append(f"ğŸ† Collectibility\n{collection_value}\n")
        if competitive_freq:
            report_lines.append(f"âš”ï¸ Competitive Frequency\n{competitive_freq}\n")
        report_lines.append("---")
        report_lines.append("ğŸ“Š Recent Sales (newest first)\nğŸ¦ PriceCharting Records")
    else:
        report_lines = []
        report_lines.append(f"# MARKET REPORT GENERATED")
        report_lines.append("")
        report_lines.append(f"âš¡ {c_name_display} ({name}) #{number}")
        report_lines.append(f"ğŸ’® ç­‰ç´šï¼š{grade}")
        category_display = "å¯¶å¯å¤¢å¡ç‰Œ" if category.lower() == "pokemon" else "èˆªæµ·ç‹å¡ç‰Œ" if category.lower() == "one piece" else category
        report_lines.append(f"ğŸ·ï¸ ç‰ˆæœ¬ï¼š{category_display}")
        report_lines.append(f"ğŸ”¢ ç·¨è™Ÿï¼š{number}")
        if release_info:
            report_lines.append(f"ğŸ“… ç™¼è¡Œï¼š{release_info}")
        if illustrator:
            report_lines.append(f"ğŸ¨ æ’ç•«å®¶ï¼š{illustrator}")
        report_lines.append("---")
        report_lines.append("\nğŸ”¥ å¸‚å ´èˆ‡æ”¶è—åˆ†æ\n")
        report_lines.append(f"ğŸ”¥ å¸‚å ´ç†±åº¦\n{market_heat}\n")
        if features:
            feat_formatted = features.replace('\\n', '\n')
            report_lines.append(f"âœ¨ å¡ç‰‡ç‰¹é»\n{feat_formatted}\n")
        if collection_value:
            report_lines.append(f"ğŸ† æ”¶è—åƒ¹å€¼\n{collection_value}\n")
        if competitive_freq:
            report_lines.append(f"âš”ï¸ ç«¶æŠ€é »ç‡\n{competitive_freq}\n")
        report_lines.append("---")
        report_lines.append("ğŸ“Š è¿‘æœŸæˆäº¤ç´€éŒ„ (ç”±æ–°åˆ°èˆŠ)\nğŸ¦ PriceCharting æˆäº¤ç´€éŒ„")
    async def _parse_d(d_str):
        d_str = d_str.strip()
        # Handle relative dates: "n åˆ†å‰", "n æ™‚é–“å‰", "n æ—¥å‰" or "n minutes ago", etc.
        if "å‰" in d_str or "ago" in d_str:
            num = int(re.search(r'\d+', d_str).group(0))
            if "åˆ†" in d_str or "minute" in d_str:
                return datetime.now() - timedelta(minutes=num)
            if "æ™‚é–“" in d_str or "hour" in d_str:
                return datetime.now() - timedelta(hours=num)
            if "æ—¥" in d_str or "day" in d_str:
                return datetime.now() - timedelta(days=num)
        
        # Handle "YYYY-MM-DD"
        try:
            return datetime.strptime(d_str, "%Y-%m-%d")
        except: pass
        
        # Handle "YYYY/MM/DD"
        try:
            return datetime.strptime(d_str, "%Y/%m/%d")
        except: pass
        
        # Handle "Jan 1, 2024"
        try:
            return datetime.strptime(d_str, "%b %d, %Y")
        except: pass
        
        return datetime.now()

    async def count_30_days(records_list, tgt_grade):
        cutoff = datetime.now() - timedelta(days=30)
        return len([r for r in (records_list or []) if r.get('grade') == tgt_grade and (await _parse_d(r['date'])) > cutoff])
    if pc_records:
        if report_pc_records:
            for r in report_pc_records[:10]:
                state_label = "Grade" if lang == "en" else "ç‹€æ…‹"
                report_lines.append(f"ğŸ“… {r['date']}      ğŸ’° ${r['price']:.2f} USD      ğŸ“ {state_label}ï¼š{r['grade']}")
            
            cutoff_12m = datetime.now() - timedelta(days=365)
            # Filter for statistics: only last 12 months
            stats_pc_records = []
            for r in report_pc_records:
                parsed_date = await _parse_d(r['date'])
                if parsed_date > cutoff_12m:
                    stats_pc_records.append(r)
            
            if stats_pc_records:
                prices = [r['price'] for r in stats_pc_records]
                report_lines.append("ğŸ“Š Statistics (Last 12 Mo.)" if lang == "en" else "ğŸ“Š çµ±è¨ˆè³‡æ–™ (è¿‘ 12 å€‹æœˆ)")
                report_lines.append(f"ã€€ğŸ’° {'Highest':}: ${max(prices):.2f} USD" if lang == "en" else f"ã€€ğŸ’° æœ€é«˜æˆäº¤åƒ¹ï¼š${max(prices):.2f} USD")
                report_lines.append(f"ã€€ğŸ’° {'Lowest':}: ${min(prices):.2f} USD" if lang == "en" else f"ã€€ğŸ’° æœ€ä½æˆäº¤åƒ¹ï¼š${min(prices):.2f} USD")
                report_lines.append(f"ã€€ğŸ’° {'Average':}: ${sum(prices)/len(prices):.2f} USD" if lang == "en" else f"ã€€ğŸ’° å¹³å‡æˆäº¤åƒ¹ï¼š${sum(prices)/len(prices):.2f} USD")
                report_lines.append(f"ã€€ğŸ“ˆ {'Records':}: {len(prices)}" if lang == "en" else f"ã€€ğŸ“ˆ è³‡æ–™ç­†æ•¸ï¼š{len(prices)} ç­†")
            else:
                report_lines.append("ğŸ“Š Statistics (No records in last 12 mo.)" if lang == "en" else "ğŸ“Š çµ±è¨ˆè³‡æ–™ (è¿‘ 12 å€‹æœˆç„¡æˆäº¤ç´€éŒ„)")
        else:
            no_data_msg = f"PriceCharting: No {grade} records found." if lang == "en" else f"PriceCharting: ç„¡ {grade} ç­‰ç´šçš„å¡ç‰‡è³‡æ–™"
            report_lines.append(no_data_msg)
    else:
        report_lines.append("PriceCharting: No data found." if lang == "en" else "PriceCharting: ç„¡æ­¤å¡ç‰‡è³‡æ–™")
    
    snkr_section_label = "\n---\nğŸ¯ SNKRDUNK Records" if lang == "en" else "\n---\nğŸ¯ SNKRDUNK æˆäº¤ç´€éŒ„"
    report_lines.append(snkr_section_label)
    if snkr_records:
        if '10' in grade:
            valid_snkr_grades = ['S', 'PSA10', 'PSA 10']
            target_disp = 'S (PSA 10)'
        elif grade.lower() == 'ungraded':
            target_disp = 'A (Raw)'
        else:
            target_disp = grade
            
        # snkr_target_records is now report_snkr_records
        if report_snkr_records:
            for r in report_snkr_records[:10]:
                usd_price = r['price'] / jpy_rate
                state_label = "Grade" if lang == "en" else "ç‹€æ…‹"
                report_lines.append(f"ğŸ“… {r['date']}      ğŸ’° Â¥{int(r['price']):,} (~${usd_price:.0f} USD)      ğŸ“ {state_label}ï¼š{r['grade']}")
            # Filter for statistics: only last 12 months
            stats_snkr_records = []
            for r in report_snkr_records:
                parsed_date = await _parse_d(r['date'])
                if parsed_date > cutoff_12m:
                    stats_snkr_records.append(r)

            if stats_snkr_records:
                prices = [r['price'] for r in stats_snkr_records]
                avg_price = sum(prices)/len(prices)
                report_lines.append("ğŸ“Š Statistics (Last 12 Mo.)" if lang == "en" else "ğŸ“Š çµ±è¨ˆè³‡æ–™ (è¿‘ 12 å€‹æœˆ)")
                report_lines.append(f"ã€€ğŸ’° {'Highest':}: Â¥{int(max(prices)):,} (~${max(prices)/jpy_rate:.0f} USD)" if lang == "en" else f"ã€€ğŸ’° æœ€é«˜æˆäº¤åƒ¹ï¼šÂ¥{int(max(prices)):,} (~${max(prices)/jpy_rate:.0f} USD)")
                report_lines.append(f"ã€€ğŸ’° {'Lowest':}: Â¥{int(min(prices)):,} (~${min(prices)/jpy_rate:.0f} USD)" if lang == "en" else f"ã€€ğŸ’° æœ€ä½æˆäº¤åƒ¹ï¼šÂ¥{int(min(prices)):,} (~${min(prices)/jpy_rate:.0f} USD)")
                report_lines.append(f"ã€€ğŸ’° {'Average':}: Â¥{int(avg_price):,} (~${avg_price/jpy_rate:.0f} USD)" if lang == "en" else f"ã€€ğŸ’° å¹³å‡æˆäº¤åƒ¹ï¼šÂ¥{int(avg_price):,} (~${avg_price/jpy_rate:.0f} USD)")
                report_lines.append(f"ã€€ğŸ“ˆ {'Records':}: {len(prices)}" if lang == "en" else f"ã€€ğŸ“ˆ è³‡æ–™ç­†æ•¸ï¼š{len(prices)} ç­†")
            else:
                report_lines.append("ğŸ“Š Statistics (No records in last 12 mo.)" if lang == "en" else "ğŸ“Š çµ±è¨ˆè³‡æ–™ (è¿‘ 12 å€‹æœˆç„¡æˆäº¤ç´€éŒ„)")
        else:
            no_data_msg = f"SNKRDUNK: No {target_disp} records found." if lang == "en" else f"SNKRDUNK: ç„¡ {target_disp} ç­‰ç´šçš„å¡ç‰‡è³‡æ–™"
            report_lines.append(no_data_msg)
    else:
        report_lines.append("SNKRDUNK: No data found." if lang == "en" else "SNKRDUNK: ç„¡æ­¤å¡ç‰‡è³‡æ–™")
        
    report_lines.append("\n---")
    if pc_url:
        view_pc = "View PriceCharting" if lang == "en" else "æŸ¥çœ‹ PriceCharting"
        report_lines.append(f"ğŸ”— [{view_pc}]({pc_url})")
    if snkr_url:
        view_snkr = "View SNKRDUNK" if lang == "en" else "æŸ¥çœ‹ SNKRDUNK"
        view_hist = "View Sales History" if lang == "en" else "æŸ¥çœ‹ SNKRDUNK éŠ·å”®æ­·å²"
        report_lines.append(f"ğŸ”— [{view_snkr}]({snkr_url})")
        report_lines.append(f"ğŸ”— [{view_hist}]({snkr_url}/sales-histories)")

    final_report = '\n'.join(report_lines)
    print(final_report, force=True)
    
    if out_dir:
        safe_name = re.sub(r'[^A-Za-z0-9]', '_', name)
        safe_num = re.sub(r'[^A-Za-z0-9]', '_', str(number))
        
        # Create dedicated folder for the card
        card_dir_name = f"{safe_name}_{safe_num}"
        dest_dir = os.path.join(out_dir, card_dir_name)
        os.makedirs(dest_dir, exist_ok=True)
        
        filename = f"PKM_Vision_{safe_name}_{safe_num}.md"
        filepath = os.path.join(dest_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(final_report)
        print(f"âœ… å ±å‘Šå·²å„²å­˜è‡³: {filepath}")
        
    if REPORT_ONLY:
        # Inject the snkrdunk image URL into the card info dictionary for Pillow to fetch
        card_info['img_url'] = img_url
        final_dest_dir = dest_dir if out_dir else '.'
        
        # We output all the scraped data to report_data.json
        # Debug step2: å„²å­˜çˆ¬èŸ²çµæœ
        _debug_log(f"Step 2 PC: {len(pc_records) if pc_records else 0} ç­†")
        _debug_log(f"Step 2 SNKR: {len(snkr_records) if snkr_records else 0} ç­†")
        _debug_save("step2_pc.json", json.dumps(pc_records or [], indent=2, ensure_ascii=False))
        _debug_save("step2_snkr.json", json.dumps(snkr_records or [], indent=2, ensure_ascii=False))

        data_dump = {
            "card_info": card_info,
            "snkr_records": snkr_records if snkr_records else [],
            "pc_records": pc_records if pc_records else []
        }
        json_path = os.path.join(final_dest_dir, "report_data.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data_dump, f, ensure_ascii=False, indent=2)
            
        # Generate the two separate HTML-based posters (Now with FULL history)
        if stream_mode:
            # â„¹ï¸ Stream Modeï¼šä¸åœ¨é€™è£¡ç­‰å¾…æµ·å ±ç”Ÿæˆï¼Œå›å‚³æ–‡å­—å ±å‘Š + æµ·å ±ç”Ÿæˆæ‰€éœ€çš„è³‡æ–™
            # Bot æ”¶åˆ°å¾Œæœƒå…ˆå‚³é€æ–‡å­—å ±å‘Šï¼Œå†å‘¼å« generate_posters() ç”Ÿæˆæµ·å ±
            poster_data = {
                "card_info": card_info,
                "snkr_records": snkr_records,
                "pc_records": pc_records,
                "out_dir": final_dest_dir,
            }
            return (final_report, poster_data)
        else:
            out_paths = await image_generator.generate_report(card_info, snkr_records, pc_records, out_dir=final_dest_dir)
            return (final_report, out_paths)
        
    # Debug step3: å„²å­˜æœ€çµ‚å ±å‘Š
    _debug_log("Step 3: å ±å‘Šç”Ÿæˆå®Œæˆ")
    _debug_save("step3_report.md", final_report)
    
    return final_report

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

async def generate_posters(poster_data):
    """
    å°‡ process_single_image(stream_mode=True) å›å‚³çš„ poster_data dict
    å‚³å…¥ï¼Œç”Ÿæˆ profile + data å…©å¼µæµ·å ±ä¸¦å›å‚³è·¯å¾‘æ¸…å–®ã€‚
    
    Bot ç”¨æ³•ï¼ˆåœ¨å‚³å®Œæ–‡å­—å ±å‘Šä¹‹å¾Œå‘¼å«ï¼‰ï¼š
        out_paths = await market_report_vision.generate_posters(poster_data)
    """
    return await image_generator.generate_report(
        poster_data["card_info"],
        poster_data["snkr_records"],
        poster_data["pc_records"],
        out_dir=poster_data["out_dir"],
    )
