import argparse
import subprocess
import re
import requests
import json
import time
import urllib.parse
import concurrent.futures
import os
import base64
import threading
import image_generator
import tempfile
from collections import deque
from dotenv import load_dotenv

load_dotenv()

REPORT_ONLY = False

_original_print = print
def print(*args, **kwargs):
    if REPORT_ONLY and not kwargs.get('force', False):
        return
    if 'force' in kwargs:
        del kwargs['force']
    _original_print(*args, **kwargs)

_jina_requests_queue = deque()
_jina_lock = threading.Lock()

def fetch_jina_markdown(target_url):
    global _jina_requests_queue
    
    # Rate Limiter: 18 requests per 60 seconds (1 minute)
    MAX_REQUESTS = 18
    WINDOW_SIZE = 60.0
    
    with _jina_lock:
        now = time.time()
        # Remove requests older than 60 seconds
        while _jina_requests_queue and now - _jina_requests_queue[0] > WINDOW_SIZE:
            _jina_requests_queue.popleft()
            
        if len(_jina_requests_queue) >= MAX_REQUESTS:
            # Calculate sleep time required to let the oldest request expire
            sleep_time = WINDOW_SIZE - (now - _jina_requests_queue[0])
            if sleep_time > 0:
                print(f"â³ Jina API rate limit approaching ({MAX_REQUESTS}/min). Pausing for {sleep_time:.1f} seconds to cool down...")
                time.sleep(sleep_time)
                
            # After sleeping, the oldest request should have expired, so we record the new "now"
            now = time.time()
            while _jina_requests_queue and now - _jina_requests_queue[0] > WINDOW_SIZE:
                _jina_requests_queue.popleft()
                
        _jina_requests_queue.append(now)

    print(f"Fetching: {target_url}...")
    jina_url = f"https://r.jina.ai/{target_url}"
    
    for attempt in range(3):
        try:
            response = requests.get(jina_url, timeout=60)
            if response.status_code == 429:
                print(f"âš ï¸ Jina ç™¼ç”Ÿ 429 é »ç‡é™åˆ¶ (å˜—è©¦ {attempt+1}/3). æš«åœ 1 ç§’å¾Œé‡è©¦...")
                time.sleep(1)
                continue
                
            response.raise_for_status()
            return response.text
            
        except requests.exceptions.RequestException as e:
            if hasattr(e, 'response') and e.response is not None and e.response.status_code == 429:
                print(f"âš ï¸ Jina ç™¼ç”Ÿ 429 é »ç‡é™åˆ¶ (å˜—è©¦ {attempt+1}/3). æš«åœ 1 ç§’å¾Œé‡è©¦...")
                time.sleep(1)
                continue
                
            print(f"Fetch error for {target_url}: {e}")
            return ""
            
    return ""

def get_exchange_rate():
    try:
        resp = requests.get("https://open.er-api.com/v6/latest/USD")
        data = resp.json()
        return data['rates']['JPY']
    except:
        return 150.0

def extract_price(price_str):
    cleaned = re.sub(r'[^\d.]', '', price_str)
    try:
        return float(cleaned)
    except:
        return 0.0

def search_pricecharting(name, number, set_code, is_alt_art=False):
    # Strip prefix like "No." (e.g. "No.025" -> "25"), then apply lstrip('0')
    _num_raw = number.split('/')[0]
    _digits_only = re.search(r'\d+', _num_raw)
    number_clean = _digits_only.group(0).lstrip('0') if _digits_only else _num_raw.lstrip('0')
    if not number_clean: number_clean = '0'
    
    # Try with set code first, if available
    queries_to_try = []
    if set_code:
        queries_to_try.append(f"{name} {set_code} {number_clean}".replace(" ", "+"))
        queries_to_try.append(f"{name} {set_code}".replace(" ", "+"))
    queries_to_try.append(f"{name} {number_clean}".replace(" ", "+"))

    md_content = ""
    search_url = ""
    
    for query in queries_to_try:
        search_url = f"https://www.pricecharting.com/search-products?q={query}&type=prices"
        md_content = fetch_jina_markdown(search_url)
        if md_content and "Search Results" in md_content or md_content and "Your search for" in md_content:
            break
        elif md_content and "PriceCharting" in md_content:
            # might have landed on product directly
            break
            
    if not md_content:
        return None, None
    
    product_url = None
    
    if "Your search for" in md_content or "Search Results" in md_content:
        urls = re.findall(r'(https://www\.pricecharting\.com/game/[^/]+/[^" )\]]+)', md_content)
        # Deduplicate while preserving order
        urls = list(dict.fromkeys(urls))
        
        valid_urls = []
        for u in urls:
            u_end = u.split('/')[-1].lower()
            # If the card name itself is in the URL, that's a good primary indicator.
            name_slug = re.sub(r'[^a-zA-Z0-9]', '-', name.lower())
            
            # Match the number strictly (e.g., "-226" at the end, or "-226-")
            if re.search(rf'(?<!\d){number_clean}(?!\d)', u_end):
                valid_urls.append(u)
            elif name_slug in u_end:
                # Less strict fallback: if the character name is in the url, but we might get the wrong set. 
                # Let's demand the set_code if number is missing.
                if set_code and set_code.lower() in u_end:
                    valid_urls.append(u)
                
        if not valid_urls:
            print(f"DEBUG: No PC product URL stringently matched the card number {number_clean} or set.")
            return None, None
            
        product_url = valid_urls[0]
        
        # Filter based on is_alt_art
        if not is_alt_art:
            for u in valid_urls:
                lower_u = u.lower()
                if "manga" not in lower_u and "parallel" not in lower_u and "alt-art" not in lower_u and "-sp" not in lower_u:
                    product_url = u
                    break
        else:
            for u in valid_urls:
                lower_u = u.lower()
                if "manga" in lower_u or "parallel" in lower_u or "alt-art" in lower_u or "-sp" in lower_u:
                    product_url = u
                    break
        
    # Final verification: Some completely unrelated cards get snagged if their ID happens to contain "226" inside it.
    if product_url:
        print(f"DEBUG: Selected PC product URL: {product_url}")
        md_content = fetch_jina_markdown(product_url)
    else:
        print(f"DEBUG: Landed directly on PC product page")
        product_url = search_url
    
    lines = md_content.split('\n')
    records = []
    
    date_regex = r'\|\s*(\d{4}-\d{2}-\d{2}|[A-Z][a-z]{2}\s\d{1,2},\s\d{4})\s*\|'
    
    for line in lines:
        if re.search(date_regex, line):
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 5:
                date_str = parts[1]
                # Find all prices in the line; use the LAST one (the actual sale price)
                # The $6/month subscribe fee may appear first in locked rows
                all_prices = re.findall(r'\$([\d,]+\.\d{2})', line)
                if not all_prices:
                    continue
                # Skip if only the subscribe fee ($6.00) found
                real_prices = [p for p in all_prices if p not in ('6.00',)]
                if not real_prices:
                    continue
                price_str = real_prices[-1]
                price_usd = float(price_str.replace(',', ''))
                
                title_clean = line.replace(" ", "").lower()
                detected_grade = None
                if "psa10" in title_clean:
                    detected_grade = "PSA 10"
                elif "psa9" in title_clean:
                    detected_grade = "PSA 9"
                elif "psa8" in title_clean:
                    detected_grade = "PSA 8"
                elif not re.search(r'(psa|bgs|cgc|grade|gem)', title_clean):
                    # Ungraded: no grading company keywords
                    # Note: 'mint' removed from exclusion since NM/Near Mint describes raw card condition
                    detected_grade = "Ungraded"
                        
                if detected_grade:
                    records.append({
                        "date": date_str,
                        "price": price_usd,
                        "grade": detected_grade
                    })

    
    # Also parse the PC bottom summary prices (e.g. "Ungraded$33.46", "PSA 10$125.00")
    # These are summary/avg prices shown at the bottom of the page
    from datetime import datetime
    today_str = datetime.now().strftime('%Y-%m-%d')
    grade_summary_map = {
        'Ungraded': 'Ungraded',
        'PSA 10': 'PSA 10',
        'PSA 9': 'PSA 9',
        'PSA 8': 'PSA 8',
    }
    existing_grades = set(r['grade'] for r in records)
    
    for line in lines:
        for grade_label, grade_key in grade_summary_map.items():
            label_nospace = grade_label.replace(' ', '')
            # Match "Ungraded$33.46" or "PSA10$125.00" style summary lines
            if re.match(rf'^{re.escape(label_nospace)}\$[\d,]+\.\d{{2}}$', line.replace(' ', '')):
                # Only add if we have no date-based records for this grade
                if grade_key not in existing_grades:
                    price_match = re.search(r'\$[\d,]+\.\d{2}', line)
                    if price_match:
                        price_usd = extract_price(price_match.group(0))
                        # Add as a single synthetic record with today's date
                        records.append({
                            "date": today_str,
                            "price": price_usd,
                            "grade": grade_key,
                            "note": "PC avg price (sold listings locked)"
                        })
                        print(f"DEBUG: Added PC summary price for {grade_key}: ${price_usd:.2f}")
    
    records.sort(key=lambda x: x['date'], reverse=True)
    resolved_url = product_url if product_url else search_url
    
    # Try to extract the card image URL from the PC product page markdown
    # Jina renders it as: ![Image N: ...](https://product-images.s3.amazonaws.com/...)
    pc_img_url = None
    img_patterns = [
        r'!\[.*?\]\((https://product-images\.s3\.amazonaws\.com/[^\)]+)\)',
        r'!\[.*?\]\((https://[^)]+\.jpg[^\)]*)\)',
        r'!\[.*?\]\((https://[^)]+\.png[^\)]*)\)',
        r'!\[.*?\]\((https://[^)]+\.webp[^\)]*)\)',
    ]
    for pat in img_patterns:
        m = re.search(pat, md_content)
        if m:
            pc_img_url = m.group(1)
            print(f"DEBUG: Found PC card image: {pc_img_url}")
            break
    
    return records, resolved_url, pc_img_url


def search_snkrdunk(en_name, jp_name, number, set_code, is_alt_art=False):
    # Strip prefix like "No." (e.g. "No.025" -> "25"), then apply lstrip('0')
    _num_raw = number.split('/')[0]
    _digits_only = re.search(r'\d+', _num_raw)
    number_clean = _digits_only.group(0).lstrip('0') if _digits_only else _num_raw.lstrip('0')
    if not number_clean: number_clean = '0'
    number_padded = number_clean.zfill(3)

    terms_to_try = []
    
    # SNKRDUNK search is highly accurate with Set Code (e.g. "ãƒ”ã‚«ãƒãƒ¥ã‚¦ S8a-G", "ãƒ”ã‚«ãƒãƒ¥ã‚¦ SV-P")
    if set_code and jp_name:
        terms_to_try.append(f"{jp_name} {set_code}")
    if set_code:
        terms_to_try.append(f"{en_name} {set_code}")
        
    if jp_name:
        terms_to_try.extend([
            f"{jp_name} {number_clean}",
            f"{jp_name} {number_padded}"
        ])
        
    terms_to_try.extend([
        f"{en_name} {number_clean}",
        f"{en_name} {number_padded}"
    ])
    
    product_id = None
    
    for term in terms_to_try:
        q = urllib.parse.quote_plus(term)
        search_url = f"https://snkrdunk.com/search?keywords={q}"
        md_content = fetch_jina_markdown(search_url)
        
        matches = re.findall(r'\[(.*?)\]\([^\)]*?/apparels/(\d+)[^\)]*?\)', md_content)
        
        seen = set()
        unique_matches = []
        for title, pid in matches:
            if pid not in seen:
                seen.add(pid)
                unique_matches.append((title, pid))
                
        filtered_by_number = []
        for title, pid in unique_matches:
            # Drop Jina image prefixes
            title_clean = re.sub(r'(?i)image\s*\d+:\s*', '', title).lower()
            # Drop all https CDN links to prevent their timestamp digits from matching the card number
            title_clean = re.sub(r'https?://[^\s()\]]+', '', title_clean)
            
            # SNKRDUNK always pads Pokemon/One Piece numbers to at least 3 digits
            # We strictly enforce the padded number to prevent matching Jina listing indices (e.g. " 4 Pikachu")
            if number_padded in title_clean or f"{number_clean}/" in title_clean:
                # If a set_code was extracted by AI, ensure it appears in the SNKRDUNK title (which always includes set codes like [SV-P 004])
                if set_code and set_code.lower() not in title_clean:
                    continue
                filtered_by_number.append((title, pid))
                
        if not filtered_by_number:
            continue # If no titles specifically have the card number, do not guess
            
        unique_matches = filtered_by_number
                
        if unique_matches:
            product_id = unique_matches[0][1] # default to first result
            
            # Filter logic
            if not is_alt_art:
                for title, pid in unique_matches:
                    lower_t = title.lower()
                    if "ã‚³ãƒŸãƒ‘ãƒ©" not in lower_t and "manga" not in lower_t and "ãƒ‘ãƒ©ãƒ¬ãƒ«" not in lower_t \
                       and "-p" not in lower_t and "-sp" not in lower_t and "parallel" not in lower_t:
                        product_id = pid
                        break
            else:
                for title, pid in unique_matches:
                    lower_t = title.lower()
                    if "ã‚³ãƒŸãƒ‘ãƒ©" in lower_t or "manga" in lower_t or "ãƒ‘ãƒ©ãƒ¬ãƒ«" in lower_t \
                       or "-p" in lower_t or "-sp" in lower_t or "parallel" in lower_t:
                        product_id = pid
                        break
            
            break
        
        time.sleep(1)
        
    if not product_id:
        return None, None, None
        
    print(f"Found SNKRDUNK Product ID: {product_id}")
    
    sales_url = f"https://snkrdunk.com/apparels/{product_id}/sales-histories"
    sales_md = fetch_jina_markdown(sales_url)
    
    img_match = re.search(r'!\[.*?\]\((https://cdn.snkrdunk.com/.*?)\)', sales_md)
    img_url = img_match.group(1) if img_match else ""
    
    records = []
    lines = sales_md.split('\n')
    date_regex = r'^(\d{4}/\d{2}/\d{2}|\d+\s*(åˆ†|æ™‚é–“|æ—¥)å‰|\d+\s+(minute|hour|day)s?\s+ago)$'
    
    for i in range(len(lines)):
        line_clean = lines[i].strip()
        
        if re.match(date_regex, line_clean, re.IGNORECASE):
            date_found = line_clean
            grade_found = ""
            price_jpy = 0
            
            for j in range(i+1, min(i+10, len(lines))):
                l_j = lines[j].strip()
                if not l_j:
                    continue
                
                if not grade_found and not re.search(r'^\d', l_j.replace(',', '')):
                    grade_found = l_j
                    continue
                    
                if grade_found and re.search(r'^\d{1,3}(,\d{3})*$', l_j):
                    price_jpy = extract_price(l_j)
                    break
                    
            if grade_found and price_jpy > 0:
                parsed_grade = grade_found.strip()
                if parsed_grade:
                    records.append({
                        "date": date_found,
                        "price": price_jpy,
                        "grade": parsed_grade
                    })
                
    resolved_url = f"https://snkrdunk.com/apparels/{product_id}" if product_id else None
                
    return records, img_url, resolved_url

def analyze_image_with_minimax(image_path, api_key):
    # æ¸…ç† API Keyï¼Œé¿å…è¤‡è£½è²¼ä¸Šæ™‚æ··å…¥éš±è—çš„æ›è¡Œæˆ–ç‰¹æ®Šå­—å…ƒ (\u2028 ç­‰) å°è‡´ \u2028 latin-1 ç·¨ç¢¼éŒ¯èª¤
    api_key = api_key.strip().replace('\u2028', '').replace('\n', '').replace('\r', '')
    # Determine MIME type
    mime = "image/jpeg"
    ext = image_path.lower().split(".")[-1]
    if ext == "png":
        mime = "image/png"
    elif ext == "webp":
        mime = "image/webp"

    # Encode image
    with open(image_path, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')

    url = "https://api.minimax.io/v1/coding_plan/vlm"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    prompt = """è«‹ä»¥ç´” JSON æ ¼å¼å›è¦†ï¼Œä¸è¦åŒ…å«ä»»ä½• markdown èªæ³• (å¦‚ ```json èµ·å§‹ç¢¼)ï¼Œåªéœ€è¼¸å‡º JSON æœ¬é«”ã€‚
ä½ æ˜¯ä¸€ä½æ–¼å¯¶å¯å¤¢å¡ç‰Œ (Pokemon TCG) é ˜åŸŸå°ˆç²¾çš„é‘‘å®šèˆ‡ä¼°åƒ¹å°ˆå®¶ã€‚è«‹åˆ†æé€™å¼µå¡ç‰‡åœ–ç‰‡ï¼Œä¸¦ç²¾æº–æå–ä»¥ä¸‹ 13 å€‹æ¬„ä½çš„è³‡è¨Šï¼š
{
  "name": "è‹±æ–‡åç¨± (å¿…å¡«ï¼Œä¾‹å¦‚ Venusaur ex æˆ– Lillie ç­‰)",
  "set_code": "ç³»åˆ—ä»£è™Ÿ (é¸å¡«ï¼Œä½æ–¼å¡ç‰Œå·¦ä¸‹æˆ–å³ä¸‹è§’ï¼Œå¦‚ SV1a, S8a-G, SV-P, 151 ç­‰ã€‚å¦‚æœæ²’æœ‰å°å‰‡ç•™ç©ºå­—ä¸²)",
  "number": "å¡ç‰‡ç·¨è™Ÿ (å¿…å¡«ï¼Œè«‹æå–ã€Œå®Œæ•´ã€å­—ä¸²ï¼ŒåŒ…å«æ–œç·šèˆ‡å‰å¾Œæ–‡å­—ï¼Œçµ•åº¦ä¸è¦è‡ªå·±å»é™¤ 0ï¼ä¾‹å¦‚ 001/015, 004/SV-P, 114/100, 077/067 ç­‰)",
  "grade": "å¡ç‰‡ç­‰ç´š (å¿…å¡«ï¼Œå¦‚æœæœ‰PSA/BGSç­‰é‘‘å®šç›’ï¼Œå°æœ‰10å°±å¡«å¦‚ PSA 10, å¦å‰‡å¦‚æœæ˜¯è£¸å¡å°±å¡« Ungraded)",
  "jp_name": "æ—¥æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "c_name": "ä¸­æ–‡åç¨± (é¸å¡«ï¼Œæ²’æœ‰è«‹ç•™ç©ºå­—ä¸²)",
  "category": "å¡ç‰‡é¡åˆ¥ (å¡«å¯« Pokemon æˆ– One Pieceï¼Œé è¨­ Pokemon)",
  "release_info": "ç™¼è¡Œå¹´ä»½èˆ‡ç³»åˆ— (å¿…å¡«ï¼Œå¾å¡ç‰Œæ¨™èªŒæˆ–ç‰¹å¾µæ¨æ–·ï¼Œå¦‚ 2023 - 151)",
  "illustrator": "æ’ç•«å®¶ (å¿…å¡«ï¼Œå·¦ä¸‹è§’æˆ–å³ä¸‹è§’çš„è‹±æ–‡åï¼Œçœ‹ä¸æ¸…å¯å¯« Unknown)",
  "market_heat": "å¸‚å ´ç†±åº¦æè¿° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡ç†ç”±è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "features": "å¡ç‰‡ç‰¹é» (å¿…å¡«ï¼ŒåŒ…å«å…¨åœ–ã€ç‰¹æ®Šå·¥è—ç­‰ï¼Œæ¯ä¸€è¡Œè«‹ç”¨ \\n æ›è¡Œå€éš”é‡é»ï¼Œè«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "collection_value": "æ”¶è—åƒ¹å€¼è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "competitive_freq": "ç«¶æŠ€é »ç‡è©•ä¼° (å¿…å¡«ï¼Œé–‹é ­å¡«å¯« High / Medium / Lowï¼Œå¾Œé¢ç™½è©±æ–‡è©•è«–è«‹å‹™å¿…ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€æ’°å¯«)",
  "is_alt_art": "æ˜¯å¦ç‚ºæ¼«ç•«èƒŒæ™¯(Manga/Comic)æˆ–ç•°åœ–(Parallel)ï¼Ÿå¸ƒæ—å€¼ true/falseã€‚è«‹æ¥µåº¦ä»”ç´°è§€å¯Ÿå¡ç‰‡çš„ã€èƒŒæ™¯ã€ï¼šå¦‚æœèƒŒæ™¯æ˜¯ä¸€æ ¼ä¸€æ ¼çš„ã€é»‘ç™½æ¼«ç•«åˆ†é¡ã€‘ï¼Œè«‹å¡« trueï¼›å¦‚æœèƒŒæ™¯åªæœ‰é–ƒé›»ã€ç‰¹æ•ˆã€æˆ–å–®ç´”å ´æ™¯ï¼Œå°±ç®—å®ƒæ˜¯ SEC ä¹Ÿæ˜¯æ™®é€šç‰ˆï¼Œã€å¿…é ˆã€å¡« falseï¼"
}"""

    payload = {
        "prompt": prompt,
        "image_url": f"data:{mime};base64,{encoded_string}"
    }

    print("--------------------------------------------------")
    print(f"ğŸ‘ï¸â€ğŸ—¨ï¸ [Minimax Vision AI] æ­£åœ¨è§£æå¡ç‰‡å½±åƒ: {image_path}...")
    
    for attempt in range(3):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            break
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Minimax API ç¶²è·¯éŒ¯èª¤ (å˜—è©¦ {attempt+1}/3): {e}")
            if attempt == 2:
                return {}
            time.sleep(2)
    if response.status_code != 200:
        print(f"API Error: è«‹æ±‚å¤±æ•— ({response.status_code})\n{response.text}")
        return None

    data = response.json()
    try:
        content = data.get('content', '')
        if not content:
            raise KeyError("content key not found or empty")
        # Clean up markdown JSON block if model still outputs it
        content = content.replace("```json", "").replace("```", "").strip()
        result = json.loads(content)
        print(f"âœ… è§£ææˆåŠŸï¼æå–åˆ°å¡ç‰‡ï¼š{result.get('name')} #{result.get('number')}\n")
        print("--- DEBUG JSON ---")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        print("------------------\n")
        return result

    except Exception as e:
        print(f"âŒ Failed to parse JSON response: {e}")
        print(f"Raw response: {data}")
        return None

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--image_path", nargs='+', required=True, help="å¡ç‰‡åœ–ç‰‡çš„æœ¬æ©Ÿè·¯å¾‘ (å¯å‚³å…¥å¤šå¼µåœ–ç‰‡)")
    parser.add_argument("--api_key", required=False, help="Minimax API Key (è‹¥æœªæŒ‡å®šï¼Œå‰‡å¾ç’°å¢ƒè®Šæ•¸ MINIMAX_API_KEY è®€å–)")
    parser.add_argument("--out_dir", required=False, help="è‹¥æŒ‡å®šï¼Œæœƒå°‡çµæœå„²å­˜è‡³çµ¦å®šçš„è³‡æ–™å¤¾")
    parser.add_argument("--report_only", action="store_true", help="è‹¥åŠ å…¥æ­¤åƒæ•¸ï¼Œå°‡åªè¼¸å‡ºæœ€çµ‚ Markdown å ±å‘Šï¼Œéš±è—æŠ“å–èˆ‡é™¤éŒ¯æ—¥èªŒ")
    
    args = parser.parse_args()
    
    global REPORT_ONLY
    REPORT_ONLY = args.report_only
    
    api_key = args.api_key or os.getenv("MINIMAX_API_KEY")
    if not api_key:
        print("âŒ Error: è«‹æä¾› --api_key åƒæ•¸ï¼Œæˆ–åœ¨ç’°å¢ƒè®Šæ•¸è¨­å®š MINIMAX_API_KEYã€‚", force=True)
        return
        
    for img_path in args.image_path:
        print(f"\n==================================================")
        print(f"ğŸ”„ é–‹å§‹è™•ç†åœ–ç‰‡: {img_path}")
        print(f"==================================================")
        await process_single_image(img_path, api_key, args.out_dir)

async def process_single_image(image_path, api_key, out_dir=None):
    if not os.path.exists(image_path):
        print(f"âŒ Error: æ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ -> {image_path}", force=True)
        return
        
    # ç¬¬ä¸€éšæ®µï¼šé€éå¤§æ¨¡å‹è¾¨è­˜åœ–ç‰‡è³‡è¨Š
    card_info = analyze_image_with_minimax(image_path, api_key)
    
    if not card_info:
        print("âŒ å¡ç‰‡å½±åƒè¾¨è­˜å¤±æ•—ï¼Œä¸­æ­¢è™•ç†æ­¤åœ–ç‰‡ã€‚", force=True)
        return
    
    # å¾ AI å›å‚³çš„ JSON æå–å¿…å‚™è³‡è¨Š
    name = card_info.get("name", "Unknown")
    set_code = card_info.get("set_code", "")
    jp_name = card_info.get("jp_name", "")
    c_name = card_info.get("c_name", "")
    number = str(card_info.get("number", "0"))
    grade = card_info.get("grade", "Ungraded")
    category = card_info.get("category", "Pokemon")
    release_info = card_info.get("release_info", "Unknown")
    illustrator = card_info.get("illustrator", "Unknown")
    market_heat = card_info.get("market_heat", "Unknown")
    features = card_info.get("features", "Unknown")
    collection_value = card_info.get("collection_value", "Unknown")
    competitive_freq = card_info.get("competitive_freq", "Unknown")
    is_alt_art = card_info.get("is_alt_art", False)
    
    # ç¬¬äºŒéšæ®µï¼šåŸ·è¡Œçˆ¬èŸ²æŠ“å–è³‡æ–™
    print("--------------------------------------------------")
    print(f"ğŸŒ æ­£åœ¨å¾ç¶²è·¯(PC & SNKRDUNK)æŠ“å–å¸‚å ´è¡Œæƒ… (ç•°åœ–/ç‰¹æ®Šç‰ˆ: {is_alt_art})...")
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_pc = executor.submit(search_pricecharting, name, number, set_code, is_alt_art)
        future_snkr = executor.submit(search_snkrdunk, name, jp_name, number, set_code, is_alt_art)
        
        pc_result = future_pc.result()
        snkr_result = future_snkr.result()
        
        pc_records = pc_result[0] if pc_result else None
        pc_url = pc_result[1] if pc_result else None
        pc_img_url = pc_result[2] if pc_result and len(pc_result) > 2 else None
        
        snkr_records = snkr_result[0] if snkr_result else None
        img_url = snkr_result[1] if snkr_result else None
        snkr_url = snkr_result[2] if snkr_result else None
    
    # Fallback: if SNKRDUNK has no image, use PriceCharting image
    if not img_url and pc_img_url:
        print(f"â„¹ï¸ SNKRDUNK ç„¡åœ–ç‰‡ï¼Œæ”¹ç”¨ PriceCharting åœ–ç‰‡ä½œç‚º fallback: {pc_img_url}")
        img_url = pc_img_url
    
    jpy_rate = get_exchange_rate()
    
    # ç¬¬ä¸‰éšæ®µï¼šç”¢ç”Ÿ Markdown å ±å‘Š
    
    c_name_display = c_name if c_name else jp_name if jp_name else name
    
    report_lines = []
    report_lines.append(f"# MARKET REPORT GENERATED")
    report_lines.append("")
    report_lines.append(f"âš¡ {c_name_display} ({name}) #{number}")
    report_lines.append(f"ğŸ’ ç­‰ç´šï¼š{grade}")
    
    category_display = "å¯¶å¯å¤¢å¡ç‰Œ" if category.lower() == "pokemon" else "èˆªæµ·ç‹å¡ç‰Œ" if category.lower() == "one piece" else category
    report_lines.append(f"ğŸ·ï¸ ç‰ˆæœ¬ï¼š{category_display}")
    
    report_lines.append(f"ğŸ”¢ ç·¨è™Ÿï¼š{number}")
    if release_info:
        report_lines.append(f"ğŸ“… ç™¼è¡Œï¼š{release_info}")
    if illustrator:
        report_lines.append(f"ğŸ¨ æ’ç•«å®¶ï¼š{illustrator}")
    
    report_lines.append("---")
    report_lines.append("\nğŸ”¥ å¸‚å ´èˆ‡æ”¶è—åˆ†æ\n")
    report_lines.append(f"ğŸ”¥ å¸‚å ´ç†±åº¦\n{market_heat}\n")
    if features:
        feat_formatted = features.replace('\\n', '\n')
        report_lines.append(f"âœ¨ å¡ç‰‡ç‰¹é»\n{feat_formatted}\n")
    if collection_value:
        report_lines.append(f"ğŸ† æ”¶è—åƒ¹å€¼\n{collection_value}\n")
    if competitive_freq:
        report_lines.append(f"âš”ï¸ ç«¶æŠ€é »ç‡\n{competitive_freq}\n")
        
    report_lines.append("---")
    
    report_lines.append("ğŸ“Š è¿‘æœŸæˆäº¤ç´€éŒ„ (ç”±æ–°åˆ°èˆŠ)\nğŸ¦ PriceCharting æˆäº¤ç´€éŒ„")
    async def count_30_days(records_list, tgt_grade):
        cutoff = datetime.now() - timedelta(days=30)
        return len([r for r in (records_list or []) if r.get('grade') == tgt_grade and (await _parse_d(r['date'])) > cutoff])
    if pc_records:
        pc_target_records = [r for r in pc_records if r['grade'] == grade]
        if pc_target_records:
            for r in pc_target_records[:10]:
                report_lines.append(f"ğŸ“… {r['date']}      ğŸ’° ${r['price']:.2f} USD      ğŸ“ ç‹€æ…‹ï¼š{r['grade']}")
            prices = [r['price'] for r in pc_target_records]
            report_lines.append("ğŸ“Š çµ±è¨ˆè³‡æ–™")
            report_lines.append(f"ã€€ğŸ’° æœ€é«˜æˆäº¤åƒ¹ï¼š${max(prices):.2f} USD")
            report_lines.append(f"ã€€ğŸ’° æœ€ä½æˆäº¤åƒ¹ï¼š${min(prices):.2f} USD")
            report_lines.append(f"ã€€ğŸ’° å¹³å‡æˆäº¤åƒ¹ï¼š${sum(prices)/len(prices):.2f} USD")
            report_lines.append(f"ã€€ğŸ“ˆ è³‡æ–™ç­†æ•¸ï¼š{len(prices)} ç­†")
        else:
            report_lines.append(f"PriceCharting: ç„¡ {grade} ç­‰ç´šçš„å¡ç‰‡è³‡æ–™")
    else:
        report_lines.append("PriceCharting: ç„¡æ­¤å¡ç‰‡è³‡æ–™")
    
    report_lines.append("\n---\nğŸ¯ SNKRDUNK æˆäº¤ç´€éŒ„")
    if snkr_records:
        if '10' in grade:
            valid_snkr_grades = ['S', 'PSA10', 'PSA 10']
            target_disp = 'S (PSA 10)'
        elif grade.lower() == 'ungraded':
            valid_snkr_grades = ['A']
            target_disp = 'A (Raw)'
        else:
            valid_snkr_grades = [grade, grade.replace(' ', '')]
            target_disp = grade
            
        snkr_target_records = [r for r in snkr_records if r['grade'] in valid_snkr_grades]
        if snkr_target_records:
            for r in snkr_target_records[:10]:
                usd_price = r['price'] / jpy_rate
                report_lines.append(f"ğŸ“… {r['date']}      ğŸ’° Â¥{int(r['price']):,} (~${usd_price:.0f} USD)      ğŸ“ ç‹€æ…‹ï¼š{r['grade']}")
            prices = [r['price'] for r in snkr_target_records]
            avg_price = sum(prices)/len(prices)
            report_lines.append("ğŸ“Š çµ±è¨ˆè³‡æ–™")
            report_lines.append(f"ã€€ğŸ’° æœ€é«˜æˆäº¤åƒ¹ï¼šÂ¥{int(max(prices)):,} (~${max(prices)/jpy_rate:.0f} USD)")
            report_lines.append(f"ã€€ğŸ’° æœ€ä½æˆäº¤åƒ¹ï¼šÂ¥{int(min(prices)):,} (~${min(prices)/jpy_rate:.0f} USD)")
            report_lines.append(f"ã€€ğŸ’° å¹³å‡æˆäº¤åƒ¹ï¼šÂ¥{int(avg_price):,} (~${avg_price/jpy_rate:.0f} USD)")
            report_lines.append(f"ã€€ğŸ“ˆ è³‡æ–™ç­†æ•¸ï¼š{len(prices)} ç­†")
        else:
            report_lines.append(f"SNKRDUNK: ç„¡ {target_disp} ç­‰ç´šçš„å¡ç‰‡è³‡æ–™")
    else:
        report_lines.append("SNKRDUNK: ç„¡æ­¤å¡ç‰‡è³‡æ–™")
        
    report_lines.append("\n---")
    if pc_url:
        report_lines.append(f"ğŸ”— [æŸ¥çœ‹ PriceCharting]({pc_url})")
    if snkr_url:
        report_lines.append(f"ğŸ”— [æŸ¥çœ‹ SNKRDUNK]({snkr_url})")
        report_lines.append(f"ğŸ”— [æŸ¥çœ‹ SNKRDUNK éŠ·å”®æ­·å²]({snkr_url}/sales-histories)")

    final_report = '\n'.join(report_lines)
    print(final_report, force=True)
    
    if out_dir:
        safe_name = re.sub(r'[^A-Za-z0-9]', '_', name)
        safe_num = re.sub(r'[^A-Za-z0-9]', '_', str(number))
        
        # Create dedicated folder for the card
        card_dir_name = f"{safe_name}_{safe_num}"
        dest_dir = os.path.join(out_dir, card_dir_name)
        os.makedirs(dest_dir, exist_ok=True)
        
        filename = f"PKM_Vision_{safe_name}_{safe_num}.md"
        filepath = os.path.join(dest_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(final_report)
        print(f"âœ… å ±å‘Šå·²å„²å­˜è‡³: {filepath}")
        
    if REPORT_ONLY:
        # Inject the snkrdunk image URL into the card info dictionary for Pillow to fetch
        card_info['img_url'] = img_url
        final_dest_dir = dest_dir if out_dir else '.'
        
        # We output all the scraped data to report_data.json
        data_dump = {
            "card_info": card_info,
            "snkr_records": snkr_records if snkr_records else [],
            "pc_records": pc_records if pc_records else []
        }
        json_path = os.path.join(final_dest_dir, "report_data.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data_dump, f, ensure_ascii=False, indent=2)
            
        # Generate the two separate HTML-based posters (Now with FULL history)
        out_paths = await image_generator.generate_report(card_info, snkr_records, pc_records, out_dir=final_dest_dir)
        
        return (final_report, out_paths)
        
    return final_report

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
